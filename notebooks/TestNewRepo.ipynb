{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../batchedRNN/newTrainUtils.py\"\n",
    "import logging, sys\n",
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.utils.data as torchUtils\n",
    "import torch.optim as optim\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from shutil import copy2, copyfile, copytree\n",
    "import argparse\n",
    "\n",
    "logging.basicConfig(stream=sys.stderr,level=logging.DEBUG)\n",
    "\n",
    "def getSaveDir():\n",
    "    saveDir = '../save/models/model0/'\n",
    "    while os.path.isdir(saveDir):\n",
    "        numStart = saveDir.rfind(\"model\")+5\n",
    "        numEnd = saveDir.rfind(\"/\")\n",
    "        saveDir = saveDir[:numStart] + str(int(saveDir[numStart:numEnd])+1) + \"/\"\n",
    "    os.mkdir(saveDir)\n",
    "    return saveDir\n",
    "\n",
    "def saveUsefulData():\n",
    "    argsFile = args.save_dir + \"args.txt\"\n",
    "    with open(argsFile, \"w\") as f:\n",
    "        f.write(json.dumps(vars(args)))\n",
    "    copy2(\"./train.py\", args.save_dir+\"train.py\")\n",
    "    copy2(\"./utils.py\", args.save_dir+\"utils.py\")\n",
    "    copy2(\"./gridSearchOptimize.py\", args.save_dir+\"gridsearchOptimize.py\")\n",
    "    copytree(\"./model\", args.save_dir+\"model/\")\n",
    "\n",
    "def getLoaderAndScaler(dataDir, category):\n",
    "    logging.info(\"Getting {} loader\".format(category))\n",
    "    f = np.load(os.path.join(dataDir, category + '.npz'))\n",
    "    my_dataset = torchUtils.TensorDataset(torch.Tensor(f[\"inputs\"]),torch.Tensor(f[\"targets\"])) # create your datset\n",
    "    scaler = getScaler(f[\"inputs\"])\n",
    "    sequence_len = f['inputs'].shape[1]\n",
    "    x_dim = f['inputs'].shape[2]\n",
    "    channels = f[\"inputs\"].shape[3]\n",
    "    shf = False\n",
    "    if category == \"train\":\n",
    "        shf = True\n",
    "    loader = torchUtils.DataLoader(\n",
    "        my_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=shf,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "        )\n",
    "    return loader, scaler, sequence_len, x_dim, channels # create your dataloader\n",
    "\n",
    "def getDataLoaders(dataDir, debug=False):\n",
    "    loaders = {}\n",
    "    logging.info(\"Getting loaders\")\n",
    "    if debug:\n",
    "        categories = [\"test\"]\n",
    "        scalerSet = \"test\"\n",
    "    else:\n",
    "        categories = [\"train\", \"val\", \"test\"]\n",
    "        scalerSet = \"train\"\n",
    "    for category in categories:\n",
    "        loader, scaler, sequence_len, x_dim, channels = getLoaderAndScaler(dataDir, category)\n",
    "        if category == scalerSet:\n",
    "            loaders[\"scaler\"] = scaler\n",
    "            loaders[\"sequence_len\"] = sequence_len\n",
    "            loaders[\"x_dim\"] = x_dim\n",
    "            loaders[\"channels\"] = channels\n",
    "        loaders[category] = loader\n",
    "    return loaders\n",
    "\n",
    "def transformBatch(batch, scaler=None):\n",
    "    x = scaler.transform(batch[0]).permute(1,0,3,2)\n",
    "    y = scaler.transform(batch[1])[...,0].permute(1,0,2)\n",
    "    return x, y\n",
    "\n",
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean0, std0, mean1=0, std1=1):\n",
    "        self.mean0 = mean0\n",
    "        self.mean1 = mean1\n",
    "        self.std0 = std0\n",
    "        self.std1 = std1\n",
    "\n",
    "    def transform(self, data):\n",
    "        mean = torch.zeros(data.size())\n",
    "        mean[...,0] = self.mean0\n",
    "        mean[...,1] = self.mean1\n",
    "        std = torch.ones(data.size())\n",
    "        std[...,0] = self.std0\n",
    "        std[...,1] = self.std1\n",
    "        return torch.div(torch.sub(data,mean),std)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        \"\"\"\n",
    "        Inverse transform is applied to output and target.\n",
    "        These are only the speeds, so only use the first \n",
    "        \"\"\"\n",
    "        mean = torch.ones(data.size()) * self.mean0\n",
    "        std = torch.ones(data.size()) * self.std0\n",
    "        transformed = torch.add(torch.mul(data, std), mean)\n",
    "        return transformed.permute(1,0,2)\n",
    "\n",
    "    def inverse_transform_both_layers(self, data):\n",
    "        mean = torch.zeros(data.size())\n",
    "        mean[...,0] = self.mean0\n",
    "        mean[...,1] = self.mean1\n",
    "        std = torch.ones(data.size())\n",
    "        std[...,0] = self.std0\n",
    "        std[...,1] = self.std1\n",
    "        transformed =  torch.add(torch.mul(data, std), mean)\n",
    "        return transformed.permute(1,0,3,2)\n",
    "\n",
    "def getScaler(trainX):\n",
    "    mean = np.mean(trainX[...,0])\n",
    "    std = np.std(trainX[...,0])\n",
    "    return StandardScaler(mean, std)\n",
    "\n",
    "def getLoss(output, target, scaler):\n",
    "    output = scaler.inverse_transform(output)\n",
    "    target = scaler.inverse_transform(target)\n",
    "    assert output.size() == target.size(), \"output size: {}, target size: {}\".format(output.size(), target.size())\n",
    "    criterion = \"RMSE\"\n",
    "    if criterion == \"RMSE\":\n",
    "        criterion = nn.MSELoss()\n",
    "        return torch.sqrt(criterion(output, target))\n",
    "    elif criterion == \"L1Loss\":\n",
    "        criterion = nn.L1Loss()\n",
    "        return criterion(output, target)\n",
    "    else:\n",
    "        assert False, \"bad loss function\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeds = (np.random.randn(5, 4,3)* 10) + 65\n",
    "timeOfDay = np.random.rand(5, 4,3)\n",
    "print(speeds)\n",
    "print(timeOfDay)\n",
    "inputData = np.stack((speeds, timeOfDay), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedsTarget = (np.random.randn(5,4,3)* 10) + 65\n",
    "timeOfDayTarget = np.random.rand(5,4,3)\n",
    "targetData = np.stack((speedsTarget, timeOfDayTarget), axis=3)\n",
    "print(targetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = getScaler(inputData, targetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.std0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.std1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTTransformed, targetTTransformed = transformBatch([torch.FloatTensor(inputData), torch.FloatTensor(targetData)], scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTTransformed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(inputTTransformed[:,:,0,:].numpy()), np.std(inputTTransformed[:,:,0,:].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTTransformed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(targetTTransformed.numpy()), np.std(targetTTransformed.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTTransformed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.concatenate((inputTTransformed.numpy()[:,:,0,:], targetTTransformed.numpy()),axis=1)),np.std(np.concatenate((inputTTransformed.numpy()[:,:,0,:], targetTTransformed.numpy()),axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(inputData[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(torch.FloatTensor(inputData))[...,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTTransformed[:,0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetTTransformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.append(torch.FloatTensor(inputData))\n",
    "data.append(torch.FloatTensor(targetData))\n",
    "transedX, transedY = transformBatch(data, scaler)\n",
    "print(transedX.shape)\n",
    "print(transedY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mockOutput = transedX[:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invTransOutput = scaler.inverse_transform(mockOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invTransOutput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invTransOutput[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(transedY)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetData[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.FloatTensor(inputData) == torch.FloatTensor(inputData).permute(1,0,3,2).permute(1,0,3,2)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLoss(mockOutput, targetTTransformed, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((inputData[...,0] - targetData[...,0])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
