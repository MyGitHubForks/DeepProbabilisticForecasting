{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/Data.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    " \n",
    "class DataLoader(object):\n",
    "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        if shuffle:\n",
    "            self.shuffle()\n",
    "\n",
    "    def get_iterator(self):\n",
    "        self.current_ind = 0\n",
    "\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = np.transpose(self.xs[start_ind: end_ind, ...], (1,0,3,2))\n",
    "                y_i = np.transpose(self.ys[start_ind: end_ind, :,:,0], (1,0,2))\n",
    "                yield (x_i, y_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "\n",
    "\n",
    "    def shuffle(self):\n",
    "        permutation = np.random.permutation(self.size)\n",
    "        self.xs, self.ys = self.xs[permutation], self.ys[permutation]\n",
    "\n",
    "class DataLoaderWithTime(object):\n",
    "    def __init__(self, xs, ys, tx, ty, batch_size, pad_with_last_sample=True, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            tx_padding = np.repeat(tx[-1:], num_padding, axis=0)\n",
    "            ty_padding = np.repeat(ty[-1:], num_padding, axis=0)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "            tx = np.concatenate([tx, tx_padding], axis=0)\n",
    "            ty = np.concatenate([ty, ty_padding], axis=0)\n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.tx = tx\n",
    "        self.ty = ty\n",
    "        if shuffle:\n",
    "            self.shuffle()\n",
    "\n",
    "    def get_iterator(self):\n",
    "        self.current_ind = 0\n",
    "\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = np.transpose(self.xs[start_ind: end_ind, ...], (1,0,3,2))\n",
    "                y_i = np.transpose(self.ys[start_ind: end_ind, :,:,0], (1,0,2))\n",
    "                tx_i = np.transpose(self.tx[start_ind: end_ind, ...], (1,0))\n",
    "                ty_i = np.transpose(self.ty[start_ind: end_ind, ...], (1,0))\n",
    "                yield (x_i, y_i, tx_i, ty_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "\n",
    "    def shuffle(self):\n",
    "        permutation = np.random.permutation(self.size)\n",
    "        self.xs, self.ys, self.tx, self.ty = self.xs[permutation], self.ys[permutation], self.tx[permutation], self.ty[permutation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/newTrainUtils.py\n",
    "import logging, sys\n",
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.utils.data as torchUtils\n",
    "import torch.optim as optim\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from shutil import copy2, copyfile, copytree\n",
    "import argparse\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Batched Sequence to Sequence')\n",
    "# parser.add_argument('--h_dim', type=int, default=256)\n",
    "# parser.add_argument(\"--z_dim\", type=int, default=0)\n",
    "# parser.add_argument('--no_cuda', action='store_true', default=False,\n",
    "#                                         help='disables CUDA training')\n",
    "# parser.add_argument(\"--no_attn\", action=\"store_true\", default=True, help=\"Do not use AttnDecoder\")\n",
    "# parser.add_argument(\"--n_epochs\", type=int, default=200)\n",
    "# parser.add_argument(\"--batch_size\", type=int, default= 64)\n",
    "# parser.add_argument(\"--n_layers\", type=int, default=2)\n",
    "# parser.add_argument(\"--initial_lr\", type=float, default=1e-4)\n",
    "# parser.add_argument(\"--lr_decay_every\", type=int, default=10)\n",
    "# parser.add_argument(\"--lr_decay_factor\", type=float, default=.10)\n",
    "# parser.add_argument(\"--lr_decay_beginning\", type=int, default=20)\n",
    "# parser.add_argument(\"--print_every\", type=int, default = 200)\n",
    "# parser.add_argument(\"--criterion\", type=str, default=\"L1Loss\")\n",
    "# parser.add_argument(\"--save_freq\", type=int, default=10)\n",
    "# parser.add_argument(\"--down_sample\", type=float, default=0.0, help=\"Keep this fraction of the training data\")\n",
    "# # parser.add_argument(\"--data_dir\", type=str, default=\"./data/reformattedTraffic/\")\n",
    "# parser.add_argument(\"--model\", type=str, default=\"sketch-rnn\")\n",
    "# parser.add_argument(\"--lambda_l1\", type=float, default=2e-5)\n",
    "# parser.add_argument(\"--lambda_l2\", type=float, default=5e-4)\n",
    "# parser.add_argument(\"--no_schedule_sampling\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--scheduling_start\", type=float, default=1.0)\n",
    "# parser.add_argument(\"--scheduling_end\", type=float, default=0.0)\n",
    "# parser.add_argument(\"--tries\", type=int, default=12)\n",
    "# parser.add_argument(\"--kld_warmup_until\", type=int, default=5)\n",
    "# parser.add_argument(\"--kld_weight_max\", type=float, default=0.10)\n",
    "# parser.add_argument(\"--no_shuffle_after_epoch\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--clip\", type=int, default=10)\n",
    "# parser.add_argument(\"--dataset\", type=str, default=\"traffic\")\n",
    "# parser.add_argument(\"--predictOnTest\", action=\"store_true\", default=True)\n",
    "# parser.add_argument(\"--encoder_input_dropout\", type=float, default=0.5)\n",
    "# parser.add_argument(\"--encoder_layer_dropout\", type=float, default=0.5)\n",
    "# parser.add_argument(\"--decoder_input_dropout\", type=float, default=0.5)\n",
    "# parser.add_argument(\"--decoder_layer_dropout\", type=float, default=0.5)\n",
    "# parser.add_argument(\"--noEarlyStopping\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--earlyStoppingPatients\", type=int, default=3)\n",
    "# parser.add_argument(\"--earlyStoppingMinDelta\", type=float, default=0.0001)\n",
    "# parser.add_argument(\"--bidirectionalEncoder\", type=bool, default=True)\n",
    "# parser.add_argument(\"--local\", action=\"store_true\", default=False)\n",
    "# args = parser.parse_args()\n",
    "logging.basicConfig(stream=sys.stderr,level=logging.DEBUG)\n",
    "\n",
    "def plotLosses(trainLosses, valLosses, trainKLDLosses=None, valKLDLosses=None):\n",
    "    torch.save(trainLosses, args.save_dir+\"plot_train_recon_losses\")\n",
    "    torch.save(valLosses, args.save_dir+\"plot_val_recon_losses\")\n",
    "    if trainKLDLosses and valKLDLosses:\n",
    "        torch.save(trainKLDLosses, args.save_dir+\"plot_train_KLD_losses\")\n",
    "        torch.save(valKLDLosses, args.save_dir+\"plot_val_KLD_losses\")\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(args.criterion, color=\"r\")\n",
    "    ax1.tick_params('y', colors='r')\n",
    "    ax1.plot(np.arange(1, len(trainLosses)+1), trainLosses, \"r--\", label=\"train reconstruction loss\")\n",
    "    ax1.plot(np.arange(1, len(valLosses)+1), valLosses, color=\"red\", label=\"validation reconstruction loss\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.grid()\n",
    "    plt.title(\"Losses for {}\".format(args.model))\n",
    "    plt.savefig(args.save_dir + \"train_val_loss_plot.png\")\n",
    "\n",
    "def getSaveDir():\n",
    "    saveDir = '../save/models/model0/'\n",
    "    while os.path.isdir(saveDir):\n",
    "        numStart = saveDir.rfind(\"model\")+5\n",
    "        numEnd = saveDir.rfind(\"/\")\n",
    "        saveDir = saveDir[:numStart] + str(int(saveDir[numStart:numEnd])+1) + \"/\"\n",
    "    os.mkdir(saveDir)\n",
    "    return saveDir\n",
    "\n",
    "def saveUsefulData():\n",
    "    argsFile = args.save_dir + \"args.txt\"\n",
    "    with open(argsFile, \"w\") as f:\n",
    "        f.write(json.dumps(vars(args)))\n",
    "    copy2(\"./train.py\", args.save_dir+\"train.py\")\n",
    "    copy2(\"./utils.py\", args.save_dir+\"utils.py\")\n",
    "    copy2(\"./gridSearchOptimize.py\", args.save_dir+\"gridsearchOptimize.py\")\n",
    "    copytree(\"./model\", args.save_dir+\"model/\")\n",
    "\n",
    "def getLoaderAndScaler(dataDir, category):\n",
    "    logging.info(\"Getting {} loader\".format(category))\n",
    "    f = np.load(os.path.join(dataDir, category + '.npz'))\n",
    "    my_dataset = torchUtils.TensorDataset(torch.Tensor(f[\"inputs\"]),torch.Tensor(f[\"targets\"])) # create your datset\n",
    "    scaler = getScaler(f[\"inputs\"])\n",
    "    sequence_len = f['inputs'].shape[1]\n",
    "    x_dim = f['inputs'].shape[2]\n",
    "    channels = f[\"inputs\"].shape[3]\n",
    "    shf = False\n",
    "    if category == \"train\":\n",
    "        shf = True\n",
    "    loader = torchUtils.DataLoader(\n",
    "        my_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=shf,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "        )\n",
    "    return loader, scaler, sequence_len, x_dim, channels # create your dataloader\n",
    "\n",
    "def getDataLoaders(dataDir, debug=False):\n",
    "    loaders = {}\n",
    "    logging.info(\"Getting loaders\")\n",
    "    if debug:\n",
    "        categories = [\"test\"]\n",
    "        scalerSet = \"test\"\n",
    "    else:\n",
    "        categories = [\"train\", \"val\", \"test\"]\n",
    "        scalerSet = \"train\"\n",
    "    for category in categories:\n",
    "        loader, scaler, sequence_len, x_dim, channels = getLoaderAndScaler(dataDir, category)\n",
    "        if category == scalerSet:\n",
    "            loaders[\"scaler\"] = scaler\n",
    "            loaders[\"sequence_len\"] = sequence_len\n",
    "            loaders[\"x_dim\"] = x_dim\n",
    "            loaders[\"channels\"] = channels\n",
    "        loaders[category] = loader\n",
    "    return loaders\n",
    "\n",
    "def transformBatch(batch, scaler=None):\n",
    "    x = scaler.transform(batch[0]).permute(1,0,3,2)\n",
    "    y = scaler.transform(batch[1])[...,0].permute(1,0,2)\n",
    "    if args.cuda:\n",
    "        return x.cuda(), y.cuda()\n",
    "    return x, y\n",
    "\n",
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean0, std0, mean1=0, std1=1):\n",
    "        self.mean0 = mean0\n",
    "        self.mean1 = mean1\n",
    "        self.std0 = std0\n",
    "        self.std1 = std1\n",
    "\n",
    "    def transform(self, data):\n",
    "        mean = torch.zeros(data.size())\n",
    "        mean[...,0] = self.mean0\n",
    "        mean[...,1] = self.mean1\n",
    "        std = torch.ones(data.size())\n",
    "        std[...,0] = self.std0\n",
    "        std[...,1] = self.std1\n",
    "        return torch.div(torch.sub(data,mean),std)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        \"\"\"\n",
    "        Inverse transform is applied to output and target.\n",
    "        These are only the speeds, so only use the first \n",
    "        \"\"\"\n",
    "        mean = torch.ones(data.size()) * self.mean0\n",
    "        std = torch.ones(data.size()) * self.std0\n",
    "        transformed = torch.add(torch.mul(data, std), mean)\n",
    "        return transformed.permute(1,0,2)\n",
    "\n",
    "    def inverse_transform_both_layers(self, data):\n",
    "        mean = torch.zeros(data.size())\n",
    "        mean[...,0] = self.mean0\n",
    "        mean[...,1] = self.mean1\n",
    "        std = torch.ones(data.size())\n",
    "        std[...,0] = self.std0\n",
    "        std[...,1] = self.std1\n",
    "        transformed =  torch.add(torch.mul(data, std), mean)\n",
    "        return transformed.permute(1,0,3,2)\n",
    "\n",
    "def getScaler(trainX):\n",
    "    mean = np.mean(trainX[...,0])\n",
    "    std = np.std(trainX[...,0])\n",
    "    return StandardScaler(mean, std)\n",
    "\n",
    "def getLoss(output, target, scaler):\n",
    "    output = scaler.inverse_transform(output)\n",
    "    target = scaler.inverse_transform(target)\n",
    "    assert output.size() == target.size(), \"output size: {}, target size: {}\".format(output.size(), target.size())\n",
    "    if args.criterion == \"RMSE\":\n",
    "        criterion = nn.MSELoss()\n",
    "        return torch.sqrt(criterion(output, target)), output, target\n",
    "    elif args.criterion == \"L1Loss\":\n",
    "        criterion = nn.L1Loss()\n",
    "        return criterion(output, target), output, target\n",
    "    else:\n",
    "        assert False, \"bad loss function\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/RoseSeq2Seq\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=2, bidirectional=True, args=None):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * self.args.channels, hidden_size, n_layers, dropout=self.args.encoder_layer_dropout, bidirectional=bidirectional)\n",
    "        self.input_dropout = nn.Dropout(p=self.args.encoder_input_dropout)\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.input_dropout(embedded)\n",
    "        embedded = torch.unsqueeze(embedded, 0)\n",
    "        embedded = embedded.view(1, self.args.batch_size, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        if self.args.bidirectionalEncoder:\n",
    "            directions = 2\n",
    "        else:\n",
    "            directions = 1\n",
    "        result = Variable(torch.zeros(self.n_layers * directions, self.args.batch_size, self.hidden_size))\n",
    "        if self.args.cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=2, args=None):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Linear(output_size, hidden_size)\n",
    "        self.input_dropout = nn.Dropout(p=self.args.decoder_input_dropout)\n",
    "        if self.args.bidirectionalEncoder:\n",
    "            directions = 2\n",
    "        else:\n",
    "            directions = 1\n",
    "        # encoder hidden is (layers * directions, batch, hidden_size)\n",
    "        # converted to (layers, batch, hidden_size * directions)\n",
    "        self.gru = nn.GRU(hidden_size, directions * hidden_size, n_layers, dropout=self.args.decoder_layer_dropout)\n",
    "        # GRU output (seq_len, batch, directions * hidden_size)\n",
    "        self.out = nn.Linear(directions * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.input_dropout(embedded)\n",
    "        embedded = F.relu(embedded)\n",
    "        embedded = torch.unsqueeze(embedded, 0)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.out(output.squeeze(0))\n",
    "        #print(\"decoder output\", output[10,31])\n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.enc = EncoderRNN(self.args.x_dim, self.args.h_dim, n_layers=self.args.n_layers, bidirectional=args.bidirectionalEncoder, args=args)\n",
    "\n",
    "        self.dec = DecoderRNN(self.args.h_dim, self.args.x_dim, n_layers=self.args.n_layers, args=args)\n",
    "\n",
    "        self.use_schedule_sampling = args.use_schedule_sampling\n",
    "        self.scheduling_start = args.scheduling_start\n",
    "        self.scheduling_end = args.scheduling_end\n",
    "\n",
    "    def _cat_directions(self, h):\n",
    "        \"\"\" If the encoder is bidirectional, do the following transformation.\n",
    "            (#directions * #layers, #batch, hidden_size) -> (#layers, #batch, #directions * hidden_size)\n",
    "        \"\"\"\n",
    "        h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
    "        return h\n",
    "\n",
    "    def parameters(self):\n",
    "        return list(self.enc.parameters()) + list(self.dec.parameters())\n",
    "\n",
    "    def scheduleSample(self, epoch):\n",
    "        eps = max(self.args.scheduling_start - \n",
    "            (self.args.scheduling_start - self.args.scheduling_end)* epoch / self.args.n_epochs,\n",
    "            self.args.scheduling_end)\n",
    "        return np.random.binomial(1, eps)\n",
    "\n",
    "    def forward(self, x, target, epoch):\n",
    "        encoder_hidden = self.enc.initHidden()\n",
    "        hs = []\n",
    "        for t in range(self.args.sequence_len):\n",
    "            encoder_output, encoder_hidden = self.enc(x[t].squeeze(), encoder_hidden)\n",
    "            hs += [encoder_output]\n",
    "        if self.args.bidirectionalEncoder:\n",
    "            decoder_hidden = self._cat_directions(encoder_hidden)\n",
    "        else:\n",
    "            decoder_hidden = encoder_hidden\n",
    "        # Prepare for Decoder\n",
    "        inp = Variable(torch.zeros(self.args.batch_size, self.args.x_dim))\n",
    "        if self.args.cuda:\n",
    "            inp = inp.cuda()\n",
    "        ys = []\n",
    "        if not self.training:\n",
    "            sample=0\n",
    "        else:\n",
    "            sample = self.scheduleSample(epoch)\n",
    "        # Decode\n",
    "        for t in range(self.args.sequence_len):\n",
    "            decoder_output, decoder_hidden = self.dec(inp, decoder_hidden)\n",
    "            if sample:\n",
    "                inp = target[t-1]\n",
    "            else:\n",
    "                inp = decoder_output\n",
    "            ys += [decoder_output]\n",
    "        return torch.cat([torch.unsqueeze(y, dim=0) for y in ys])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "class Bunch(object):\n",
    "    def __init__(self, adict):\n",
    "        self.__dict__.update(adict)\n",
    "    def __str__(self):\n",
    "        out = \"\"\n",
    "        for k, v in self.__dict__.items():\n",
    "            out += \"{}: {}, \".format(k, v)\n",
    "        return out\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcess(object):\n",
    "    def __init__(self, modelPath, chooseModel=\"rnn\", dataDict=None):\n",
    "        self.modelPath = modelPath\n",
    "        argsStr= \"\"\n",
    "        with open(modelPath + \"args.txt\") as f:\n",
    "            argsStr = f.read()\n",
    "        argsD = json.loads(argsStr)\n",
    "        self.args = Bunch(argsD)\n",
    "        self.args.dropout= 0.0\n",
    "        self.args.no_cuda = True\n",
    "        self.args.cuda = False\n",
    "        if dataDict:\n",
    "            self.dataDict = dataDict\n",
    "        else:\n",
    "            self.dataDict = GetDataLoader(\"../data/traffic/trafficWithTime/\")\n",
    "        if chooseModel==\"rnn\":\n",
    "            self.model = Seq2Seq(self.args)\n",
    "        elif chooseModel==\"sketch-rnn\":\n",
    "            self.model = SketchRNN(self.args)\n",
    "        self.model.eval()\n",
    "\n",
    "    def getLoss(self, output, target):\n",
    "        output = self.dataDict[\"scaler\"].inverse_transform(output)\n",
    "        target = self.dataDict[\"scaler\"].inverse_transform(target)\n",
    "        assert output.size() == target.size(), \"output size: {}, target size: {}\".format(output.size(), target.size())\n",
    "        outputs = {}\n",
    "        mse = nn.MSELoss()\n",
    "        outputs[\"RMSE\"] = torch.sqrt(mse(output, target)).item()\n",
    "        l1loss = nn.L1Loss()\n",
    "        outputs[\"L1Loss\"] = l1loss(output, target).item()\n",
    "        outputs[\"pred\"] = output\n",
    "        outputs[\"target\"] = target\n",
    "        return outputs\n",
    "\n",
    "    def getEpochLoss(self, dataset, epoch):\n",
    "        datas = []\n",
    "        preds = []\n",
    "        targets = []\n",
    "        epoch_val_loss = 0.0\n",
    "        nValBatches = 0\n",
    "        callableTransform = partial(transformBatch, scaler=self.dataDict[\"scaler\"])\n",
    "        with torch.no_grad():\n",
    "            for batchIDX, (inputData, target) in enumerate(map(callableTransform, self.dataDict[dataset])):\n",
    "                nValBatches += 1\n",
    "                output = self.model(inputData, target, epoch)\n",
    "                lossOutputs = self.getLoss(output, target)\n",
    "                if args.criterion == \"RMSE\":\n",
    "                    epoch_val_loss += lossOutputs[\"RMSE\"]\n",
    "                elif args.criterion == \"L1Loss\":\n",
    "                    epoch_val_loss += lossOutputs[\"L1Loss\"]\n",
    "                preds.append(lossOutputs[\"pred\"].cpu().detach().numpy())\n",
    "                targets.append(lossOutputs[\"target\"].cpu().detach().numpy())\n",
    "                datas.append(self.dataDict[\"scaler\"].inverse_transform(inputData[:,:,0,:]).cpu().detach().numpy())\n",
    "        datas = np.concatenate(datas, axis=0)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        targets = np.concatenate(targets, axis=0)\n",
    "        retVals = {\n",
    "            args.criterion : epoch_val_loss / nValBatches,\n",
    "            \"preds\" : preds,\n",
    "            \"targets\" : targets,\n",
    "            \"datas\" : datas,\n",
    "        }\n",
    "        return retVals\n",
    "\n",
    "    def prep(self, stateDictFile, dataset, epoch):\n",
    "        desired_state_dict = torch.load(self.modelPath +stateDictFile, map_location=lambda storage, loc: storage)\n",
    "        self.model.load_state_dict(desired_state_dict)\n",
    "        self.model.eval()\n",
    "        assert dataset in [\"train\", \"val\", \"test\"]\n",
    "        \n",
    "\n",
    "    def getLossAtEpoch(self, stateDictFile, dataset, epoch= -1):\n",
    "        self.prep(stateDictFile, dataset, epoch)\n",
    "        retVals = self.getEpochLoss(dataset, epoch)\n",
    "        return retVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainValCurve(trainLosses, valLosses, trainKLDLosses=None, valKLDLosses=None):\n",
    "    plot_every = 1\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    plt.figure()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(args.criterion, color=\"r\")\n",
    "    ax1.tick_params('y', colors='r')\n",
    "    ax1.plot(np.arange(1, len(trainLosses)+1)*plot_every, trainLosses, \"r--\", label=\"train reconstruction loss\")\n",
    "    ax1.plot(np.arange(1, len(valLosses)+1)*plot_every, valLosses, color=\"red\", label=\"validation reconstruction loss\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.grid()\n",
    "    if trainKLDLosses:\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel(\"KLD Loss\", color=\"b\")\n",
    "        ax2.tick_params('y', colors='b')\n",
    "        ax2.plot(np.arange(1, len(trainKLDLosses)+1)*plot_every, trainKLDLosses, \"b--\", label=\"train KLD loss\")\n",
    "        ax2.plot(np.arange(1, len(valKLDLosses)+1)*plot_every, valKLDLosses, color=\"blue\", label=\"val KLD loss\")\n",
    "        ax2.legend(loc=\"upper right\")\n",
    "        ax2.grid()\n",
    "    plt.title(\"Losses for {}\".format(args.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotNHours(means, stds, targets, datas, dataset, dataMean, dataStd, targetTimes, N=24):\n",
    "    assert False, \"Need to fix\"\n",
    "    instance = np.random.randint(targets.shape[1])\n",
    "    sensor = np.random.randint(targets.shape[2])\n",
    "    sequenceTrueMean = []\n",
    "    sequenceTrueStd = []\n",
    "    sequenceSampleMean = []\n",
    "    sequenceSampleStd = []\n",
    "    sequenceTarget = []\n",
    "    sequenceTimes = []\n",
    "    shouldMask = []\n",
    "    maskindex = []\n",
    "    lastTime = None\n",
    "    for tStep in range(N):\n",
    "        realIndex = instance + 12 * tStep\n",
    "        if realIndex >= means.shape[1]:\n",
    "            break\n",
    "        if lastTime and inMinutes(targetTimes[realIndex, -1] - lastTime) > 5:\n",
    "            shouldMask += [True]\n",
    "        else:\n",
    "            shouldMask += [False]\n",
    "        lastTime = targetTimes[realIndex, -1]\n",
    "        maskindex += [len(sequenceTrueMean)]\n",
    "        m = means[:, realIndex, sensor]\n",
    "        std = stds[:, realIndex, sensor]\n",
    "        predSamples, sampleMean, sampleStd = getScaledSamples(m, std, dataMean, dataStd)\n",
    "        sequenceTrueMean += list(m)\n",
    "        sequenceTrueStd += list(std)\n",
    "        sequenceSampleMean += list(sampleMean)\n",
    "        sequenceSampleStd += list(sampleStd)\n",
    "        sequenceTarget += list(targets[:, realIndex, sensor])\n",
    "        sequenceTimes += list(targetTimes[realIndex])\n",
    "        \n",
    "    #f, ax = plt.subplots(2, sharex=True)\n",
    "    #f.subplots_adjust(hspace=.5)\n",
    "    \"\"\"\n",
    "    maskedSampleMean = ma.array(sequenceSampleMean)\n",
    "    maskedTarget = ma.array(sequenceTarget)\n",
    "    print(maskindex)\n",
    "    print(shouldMask)\n",
    "    print(maskedSampleMean.shape)\n",
    "    for idx, should in zip(maskindex, shouldMask):\n",
    "        if should:\n",
    "            maskedSampleMean[idx] = ma.masked\n",
    "            maskedTarget[idx] = ma.masked\n",
    "    \"\"\"\n",
    "    #print(np.max(sequenceSampleStd), sequenceTimes[np.argmax(sequenceSampleStd)])\n",
    "    #print(sequenceSampleMean)\n",
    "    f, ax = plt.subplots()\n",
    "    f.set_figwidth(15)\n",
    "    plt.plot(sequenceTimes, sequenceSampleMean, label=\"pred\")\n",
    "    plt.plot(sequenceTimes, sequenceTarget, label=\"target\")\n",
    "    plt.fill_between(sequenceTimes,np.array(sequenceSampleMean)-1.96*np.array(sequenceSampleStd), np.array(sequenceSampleMean)+1.96*np.array(sequenceSampleStd), alpha=0.5)\n",
    "    plt.xticks(rotation=90)\n",
    "    xfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')\n",
    "    ax=plt.gca()\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"mile/h\")\n",
    "    plt.title(\"{} Hour Sample Prediction {}\".format(N, dataset))\n",
    "    yMin = np.min((np.min(sequenceSampleMean)-10, np.min(sequenceTarget)-10, 10))\n",
    "    yMax = np.max((np.max(sequenceSampleMean)+10, np.max(sequenceTarget)+10, 70))\n",
    "    #plt.ylim((yMin,yMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
