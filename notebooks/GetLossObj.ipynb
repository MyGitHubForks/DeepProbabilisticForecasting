{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/Data.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    " \n",
    "class DataLoader(object):\n",
    "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        if shuffle:\n",
    "            self.shuffle()\n",
    "\n",
    "    def get_iterator(self):\n",
    "        self.current_ind = 0\n",
    "\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = np.transpose(self.xs[start_ind: end_ind, ...], (1,0,3,2))\n",
    "                y_i = np.transpose(self.ys[start_ind: end_ind, :,:,0], (1,0,2))\n",
    "                yield (x_i, y_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "\n",
    "\n",
    "    def shuffle(self):\n",
    "        permutation = np.random.permutation(self.size)\n",
    "        self.xs, self.ys = self.xs[permutation], self.ys[permutation]\n",
    "\n",
    "class DataLoaderWithTime(object):\n",
    "    def __init__(self, xs, ys, tx, ty, batch_size, pad_with_last_sample=True, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.current_ind = 0\n",
    "        if pad_with_last_sample:\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\n",
    "            tx_padding = np.repeat(tx[-1:], num_padding, axis=0)\n",
    "            ty_padding = np.repeat(ty[-1:], num_padding, axis=0)\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\n",
    "            tx = np.concatenate([tx, tx_padding], axis=0)\n",
    "            ty = np.concatenate([ty, ty_padding], axis=0)\n",
    "        self.size = len(xs)\n",
    "        self.num_batch = int(self.size // self.batch_size)\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.tx = tx\n",
    "        self.ty = ty\n",
    "        if shuffle:\n",
    "            self.shuffle()\n",
    "\n",
    "    def get_iterator(self):\n",
    "        self.current_ind = 0\n",
    "\n",
    "        def _wrapper():\n",
    "            while self.current_ind < self.num_batch:\n",
    "                start_ind = self.batch_size * self.current_ind\n",
    "                end_ind = min(self.size, self.batch_size * (self.current_ind + 1))\n",
    "                x_i = np.transpose(self.xs[start_ind: end_ind, ...], (1,0,3,2))\n",
    "                y_i = np.transpose(self.ys[start_ind: end_ind, :,:,0], (1,0,2))\n",
    "                tx_i = np.transpose(self.tx[start_ind: end_ind, ...], (1,0))\n",
    "                ty_i = np.transpose(self.ty[start_ind: end_ind, ...], (1,0))\n",
    "                yield (x_i, y_i, tx_i, ty_i)\n",
    "                self.current_ind += 1\n",
    "\n",
    "        return _wrapper()\n",
    "\n",
    "    def shuffle(self):\n",
    "        permutation = np.random.permutation(self.size)\n",
    "        self.xs, self.ys, self.tx, self.ty = self.xs[permutation], self.ys[permutation], self.tx[permutation], self.ty[permutation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/utils.py\n",
    "import logging, sys\n",
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.utils.data as torchUtils\n",
    "import torch.optim as optim\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from shutil import copy2, copyfile, copytree\n",
    "import argparse\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Batched Sequence to Sequence')\n",
    "# parser.add_argument('--h_dim', type=int, default=256)\n",
    "# parser.add_argument(\"--z_dim\", type=int, default=128)\n",
    "# parser.add_argument('--no_cuda', action='store_true', default=False,\n",
    "#                                         help='disables CUDA training')\n",
    "# parser.add_argument(\"--no_attn\", action=\"store_true\", default=True, help=\"Do not use AttnDecoder\")\n",
    "# parser.add_argument(\"--n_epochs\", type=int, default=200)\n",
    "# parser.add_argument(\"--batch_size\", type=int, default= 64)\n",
    "# parser.add_argument(\"--n_layers\", type=int, default=2)\n",
    "# parser.add_argument(\"--initial_lr\", type=float, default=1e-4)\n",
    "# parser.add_argument(\"--lr_decay_every\", type=int, default=10)\n",
    "# parser.add_argument(\"--lr_decay_factor\", type=float, default=.10)\n",
    "# parser.add_argument(\"--lr_decay_beginning\", type=int, default=20)\n",
    "# parser.add_argument(\"--print_every\", type=int, default = 200)\n",
    "# parser.add_argument(\"--criterion\", type=str, default=\"L1Loss\")\n",
    "# parser.add_argument(\"--save_freq\", type=int, default=10)\n",
    "# parser.add_argument(\"--down_sample\", type=float, default=0.0, help=\"Keep this fraction of the training data\")\n",
    "# # parser.add_argument(\"--data_dir\", type=str, default=\"./data/reformattedTraffic/\")\n",
    "# parser.add_argument(\"--model\", type=str, default=\"sketch-rnn\")\n",
    "# parser.add_argument(\"--lambda_l1\", type=float, default=0)\n",
    "# parser.add_argument(\"--lambda_l2\", type=float, default=5e-4)\n",
    "# parser.add_argument(\"--no_schedule_sampling\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--scheduling_start\", type=float, default=1.0)\n",
    "# parser.add_argument(\"--scheduling_end\", type=float, default=0.0)\n",
    "# parser.add_argument(\"--tries\", type=int, default=12)\n",
    "# parser.add_argument(\"--kld_warmup_until\", type=int, default=5)\n",
    "# parser.add_argument(\"--kld_weight_max\", type=float, default=0.10)\n",
    "# parser.add_argument(\"--no_shuffle_after_epoch\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--clip\", type=int, default=10)\n",
    "# parser.add_argument(\"--dataset\", type=str, default=\"traffic\")\n",
    "# parser.add_argument(\"--predictOnTest\", action=\"store_true\", default=True)\n",
    "# parser.add_argument(\"--encoder_input_dropout\", type=float, default=0.5)\n",
    "# parser.add_argument(\"--encoder_layer_dropout\", type=float, default=0.5)\n",
    "# parser.add_argument(\"--decoder_input_dropout\", type=float, default=0.5)\n",
    "# parser.add_argument(\"--decoder_layer_dropout\", type=float, default=0.5)\n",
    "# parser.add_argument(\"--noEarlyStopping\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--earlyStoppingPatients\", type=int, default=3)\n",
    "# parser.add_argument(\"--earlyStoppingMinDelta\", type=float, default=0.0001)\n",
    "# parser.add_argument(\"--bidirectionalEncoder\", type=bool, default=True)\n",
    "# parser.add_argument(\"--local\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--debugDataset\", action=\"store_true\", default=False)\n",
    "# parser.add_argument(\"--encoder_h_dim\", type=int, default=256)\n",
    "# parser.add_argument(\"--decoder_h_dim\", type=int, default=512)\n",
    "# parser.add_argument(\"--num_mixtures\", type=int, default=20)\n",
    "# args = parser.parse_args()\n",
    "# logging.basicConfig(stream=sys.stderr,level=logging.DEBUG)\n",
    "\n",
    "def plotLosses(trainLosses, valLosses, trainKLDLosses=None, valKLDLosses=None):\n",
    "    torch.save(trainLosses, args.save_dir+\"plot_train_recon_losses\")\n",
    "    torch.save(valLosses, args.save_dir+\"plot_val_recon_losses\")\n",
    "    if trainKLDLosses and valKLDLosses:\n",
    "        torch.save(trainKLDLosses, args.save_dir+\"plot_train_KLD_losses\")\n",
    "        torch.save(valKLDLosses, args.save_dir+\"plot_val_KLD_losses\")\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(args.criterion, color=\"r\")\n",
    "    ax1.tick_params('y', colors='r')\n",
    "    ax1.plot(np.arange(1, len(trainLosses)+1), trainLosses, \"r--\", label=\"train reconstruction loss\")\n",
    "    ax1.plot(np.arange(1, len(valLosses)+1), valLosses, color=\"red\", label=\"validation reconstruction loss\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.grid()\n",
    "    plt.title(\"Losses for {}\".format(args.model))\n",
    "    plt.savefig(args.save_dir + \"train_val_loss_plot.png\")\n",
    "\n",
    "def getSaveDir():\n",
    "    if args.local:\n",
    "        saveDir = '../save/local/models/model0/'\n",
    "    else:\n",
    "        saveDir = '../save/models/model0/'\n",
    "    while os.path.isdir(saveDir):\n",
    "        numStart = saveDir.rfind(\"model\")+5\n",
    "        numEnd = saveDir.rfind(\"/\")\n",
    "        saveDir = saveDir[:numStart] + str(int(saveDir[numStart:numEnd])+1) + \"/\"\n",
    "    os.mkdir(saveDir)\n",
    "    return saveDir\n",
    "\n",
    "def saveUsefulData():\n",
    "    argsFile = args.save_dir + \"args.txt\"\n",
    "    with open(argsFile, \"w\") as f:\n",
    "        f.write(json.dumps(vars(args)))\n",
    "    copy2(\"./train.py\", args.save_dir+\"train.py\")\n",
    "    copy2(\"./utils.py\", args.save_dir+\"utils.py\")\n",
    "    copy2(\"./gridSearchOptimize.py\", args.save_dir+\"gridsearchOptimize.py\")\n",
    "    copytree(\"./model\", args.save_dir+\"model/\")\n",
    "\n",
    "def getTrafficDataset(dataDir, category):\n",
    "    f = np.load(os.path.join(dataDir, category + '.npz'))\n",
    "    my_dataset = torchUtils.TensorDataset(torch.Tensor(f[\"inputs\"]),torch.Tensor(f[\"targets\"])) # create your datset\n",
    "    scaler = getScaler(f[\"inputs\"])\n",
    "    sequence_len = f['inputs'].shape[1]\n",
    "    x_dim = f['inputs'].shape[2]\n",
    "    channels = f[\"inputs\"].shape[3]\n",
    "    return my_dataset, scaler, sequence_len, sequence_len, x_dim, channels\n",
    "\n",
    "def getHumanDataset(dataDir, category):\n",
    "    f = h5py.File(os.path.join(dataDir, category+\".h5\"), \"r\")\n",
    "    my_dataset = torchUtils.TensorDataset(torch.Tensor(f[\"input2d\"]), torch.Tensor(f[\"target2d\"]))\n",
    "    scaler = getScaler(f[\"input2d\"])\n",
    "    input_sequence_len = f[\"input2d\"].shape[1]\n",
    "    target_sequence_len = f[\"target2d\"].shape[1]\n",
    "    x_dim = f[\"input2d\"].shape[2]\n",
    "    channels = f[\"input2d\"].shape[3]\n",
    "    return my_dataset, scaler, input_sequence_len, target_sequence_len, x_dim, channels\n",
    "\n",
    "def getLoaderAndScaler(dataDir, category):\n",
    "    logging.info(\"Getting {} loader\".format(category))\n",
    "    if args.dataset == \"traffic\":\n",
    "        my_dataset, scaler, input_sequence_len, target_sequence_len, x_dim, channels = getTrafficDataset(dataDir, category)\n",
    "    else:\n",
    "        my_dataset, scaler, input_sequence_len, target_sequence_len, x_dim, channels = getHumanDataset(dataDir, category)\n",
    "    shf = False\n",
    "    if category == \"train\":\n",
    "        shf = True\n",
    "    loader = torchUtils.DataLoader(\n",
    "        my_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=True\n",
    "        )\n",
    "    return loader, scaler, input_sequence_len, target_sequence_len, x_dim, channels # create your dataloader\n",
    "\n",
    "def getDataLoaders(dataDir, debug=False):\n",
    "    loaders = {}\n",
    "    logging.info(\"Getting data from {}\".format(dataDir))\n",
    "    if debug:\n",
    "        categories = [\"test\"]\n",
    "        scalerSet = \"test\"\n",
    "    else:\n",
    "        categories = [\"train\", \"val\", \"test\"]\n",
    "        scalerSet = \"train\"\n",
    "    for category in categories:\n",
    "        loader, scaler, input_sequence_len, target_sequence_len, x_dim, channels = getLoaderAndScaler(dataDir, category)\n",
    "        if category == scalerSet:\n",
    "            loaders[\"scaler\"] = scaler\n",
    "            loaders[\"input_sequence_len\"] = input_sequence_len\n",
    "            loaders[\"target_sequence_len\"] = target_sequence_len\n",
    "            loaders[\"x_dim\"] = x_dim\n",
    "            loaders[\"channels\"] = channels\n",
    "        loaders[category] = loader\n",
    "    return loaders\n",
    "\n",
    "class StandardScaler:\n",
    "    \"\"\"\n",
    "    Standard the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean0, std0, mean1, std1):\n",
    "        self.mean0 = mean0\n",
    "        self.std0 = std0\n",
    "        self.mean1 = mean1\n",
    "        self.std1 = std1\n",
    "\n",
    "    def transform(self, data):\n",
    "        mean = torch.zeros(data.size())\n",
    "        mean[...,0] = self.mean0\n",
    "        mean[...,1] = self.mean1\n",
    "        std = torch.ones(data.size())\n",
    "        std[...,0] = self.std0\n",
    "        std[...,1] = self.std1\n",
    "        return torch.div(torch.sub(data,mean),std)\n",
    "\n",
    "class StandardScalerTraffic(StandardScaler):\n",
    "    def __init__(self, mean0, std0):\n",
    "        super(StandardScalerTraffic, self).__init__(mean0, std0, 0.0, 1.0)\n",
    "\n",
    "    def inverse_transform(self, data, permute=True):\n",
    "        \"\"\"\n",
    "        Inverse transform is applied to output and target.\n",
    "        These are only the speeds, so only use the first \n",
    "        \"\"\"\n",
    "        mean = torch.ones(data.size()) * self.mean0\n",
    "        std = torch.ones(data.size()) * self.std0\n",
    "        if args.cuda:\n",
    "            mean = mean.cuda()\n",
    "            std = std.cuda()\n",
    "        transformed = torch.add(torch.mul(data, std), mean)\n",
    "        del mean, std\n",
    "        if permute:\n",
    "            return transformed.permute(1,0,2)\n",
    "        else:\n",
    "            return transformed\n",
    "\n",
    "    def transformBatchForEpoch(self, batch):\n",
    "        x = self.transform(batch[0]).permute(1,0,3,2)\n",
    "        y = self.transform(batch[1])[...,0].permute(1,0,2)\n",
    "        if args.cuda:\n",
    "            return x.cuda(), y.cuda()\n",
    "        return x, y\n",
    "\n",
    "class StandardScalerHuman(StandardScaler):\n",
    "    \"\"\"docstring for StandardScalerHuman\"\"\"\n",
    "    def __init__(self, mean0, std0, mean1, std1):\n",
    "        super(StandardScalerHuman, self).__init__(mean0, std0, mean1, std1)\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        \"\"\"\n",
    "        applied to output and target\n",
    "        \"\"\"\n",
    "        transed = self.restoreDim(data)\n",
    "        mean = torch.zeros(transed.size())\n",
    "        std = torch.ones(transed.size())\n",
    "        if args.cuda:\n",
    "            mean = mean.cuda()\n",
    "            std = std.cuda()\n",
    "        mean[...,0] = self.mean0\n",
    "        mean[...,1] = self.mean1\n",
    "        std[...,0] = self.std0\n",
    "        std[...,1] = self.std1\n",
    "        transformed =  torch.add(torch.mul(transed, std), mean)\n",
    "        del mean, std\n",
    "        return transformed.permute(1,0,3,2)\n",
    "\n",
    "    def restoreDim(self, data):\n",
    "        l1, l2 = torch.split(data, int(data.size(2) / 2), 2)\n",
    "        return torch.cat((l1.unsqueeze(3), l2.unsqueeze(3)), dim=3)\n",
    "\n",
    "    def removeDim(self, data):\n",
    "        layer0, layer1 = torch.split(data, 1, dim=3)\n",
    "        return torch.cat((layer0.squeeze(3), layer1.squeeze(3)), dim=2)\n",
    "\n",
    "    def transformBatchForEpoch(self, batch):\n",
    "        x = self.transform(batch[0]).permute(1,0,3,2)\n",
    "        y = self.transform(batch[1])\n",
    "        wideY = self.removeDim(y).permute(1,0,2)\n",
    "        if args.cuda:\n",
    "            return x.cuda(), wideY.cuda()\n",
    "        return x, wideY\n",
    "\n",
    "def getScaler(trainX):\n",
    "    mean0 = np.mean(trainX[...,0])\n",
    "    std0 = np.std(trainX[...,0])\n",
    "    mean1 = np.mean(trainX[...,1])\n",
    "    std1 = np.std(trainX[...,1])\n",
    "    if args.dataset == \"traffic\":\n",
    "        return StandardScalerTraffic(mean0, std0)\n",
    "    elif args.dataset == \"human\":\n",
    "        return StandardScalerHuman(mean0, std0, mean1, std1)\n",
    "    else:\n",
    "        assert False, \"bad dataset\"\n",
    "\n",
    "def getReconLoss(output, target, scaler):\n",
    "    output = scaler.inverse_transform(output)\n",
    "    target = scaler.inverse_transform(target)\n",
    "    assert output.size() == target.size(), \"output size: {}, target size: {}\".format(output.size(), target.size())\n",
    "    if args.criterion == \"RMSE\":\n",
    "        criterion = nn.MSELoss()\n",
    "        return torch.sqrt(criterion(output, target))\n",
    "    elif args.criterion == \"L1Loss\":\n",
    "        criterion = nn.L1Loss()\n",
    "        return criterion(output, target)\n",
    "    else:\n",
    "        assert False, \"bad loss function\"\n",
    "\n",
    "def getKLDWeight(epoch):\n",
    "    # kldLossWeight = args.kld_weight_max * min((epoch / (args.kld_warmup_until)), 1.0)\n",
    "    kldLossWeight = args.kld_weight_max\n",
    "    return kldLossWeight\n",
    "\n",
    "def kld_gauss(mean_1, std_1, mean_2, std_2):\n",
    "    \"\"\"Using std to compute KLD\"\"\"\n",
    "\n",
    "    kld_element = (2 * torch.log(std_2) - 2 * torch.log(std_1) +\n",
    "                   (std_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
    "                   std_2.pow(2) - 1)\n",
    "    return 0.5 * torch.sum(kld_element)\n",
    "\n",
    "def sketchRNNKLD(latentMean, latentStd):\n",
    "    m2 = torch.zeros_like(latentMean)\n",
    "    s2 = torch.ones_like(latentStd)\n",
    "    return kld_gauss(latentMean, latentStd, m2, s2)\n",
    "\n",
    "def getLoss(model, output, target, scaler, epoch):\n",
    "    if args.model == \"rnn\":\n",
    "        reconLoss = getReconLoss(output, target, scaler)\n",
    "        return reconLoss, 0\n",
    "    else:\n",
    "        latentMean, latentStd, z, predOut, predMeanOut, predStdOut = output\n",
    "        reconLoss = getReconLoss(predOut, target, scaler)\n",
    "        kldLoss = sketchRNNKLD(latentMean, latentStd)\n",
    "        return reconLoss, kldLoss\n",
    "\n",
    "def saveModel(modelWeights, epoch):\n",
    "    fn = args.save_dir+'{}_state_dict_'.format(args.model)+str(epoch)+'.pth'\n",
    "    torch.save(modelWeights, fn)\n",
    "    logging.info('Saved model to '+fn)\n",
    "\n",
    "class EarlyStoppingObject(object):\n",
    "    \"\"\"docstring for EarlyStoppingObject\"\"\"\n",
    "    def __init__(self):\n",
    "        super(EarlyStoppingObject, self).__init__()\n",
    "        self.bestLoss = None\n",
    "        self.bestEpoch = None\n",
    "        self.counter = 0\n",
    "        self.epochCounter = 0\n",
    "\n",
    "    def checkStop(self, previousLoss):\n",
    "        self.epochCounter += 1\n",
    "        if not args.noEarlyStopping:\n",
    "            if self.bestLoss is not None and previousLoss + args.earlyStoppingMinDelta >= self.bestLoss:\n",
    "                self.counter += 1\n",
    "                if self.counter >= args.earlyStoppingPatients:\n",
    "                    logging.info(\"Stopping Early, haven't beaten best loss {:.4f} @ Epoch {} in {} epochs\".format(\n",
    "                        self.bestLoss,\n",
    "                        self.bestEpoch,\n",
    "                        args.earlyStoppingPatients))\n",
    "                    return True\n",
    "            else:\n",
    "                self.bestLoss = previousLoss\n",
    "                self.bestEpoch = self.epochCounter\n",
    "                self.counter = 0\n",
    "                return False\n",
    "\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/RoseSeq2Seq\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=2, bidirectional=True, args=None):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_size = input_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * self.args.channels, hidden_size, n_layers, dropout=self.args.encoder_layer_dropout, bidirectional=bidirectional)\n",
    "        self.input_dropout = nn.Dropout(p=self.args.encoder_input_dropout)\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.input_dropout(embedded)\n",
    "        embedded = torch.unsqueeze(embedded, 0)\n",
    "        embedded = embedded.view(1, self.args.batch_size, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        if self.args.bidirectionalEncoder:\n",
    "            directions = 2\n",
    "        else:\n",
    "            directions = 1\n",
    "        result = Variable(torch.zeros(self.n_layers * directions, self.args.batch_size, self.hidden_size))\n",
    "        if self.args.cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=2, args=None):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Linear(output_size, hidden_size)\n",
    "        self.input_dropout = nn.Dropout(p=self.args.decoder_input_dropout)\n",
    "        if self.args.bidirectionalEncoder:\n",
    "            directions = 2\n",
    "        else:\n",
    "            directions = 1\n",
    "        # encoder hidden is (layers * directions, batch, hidden_size)\n",
    "        # converted to (layers, batch, hidden_size * directions)\n",
    "        self.gru = nn.GRU(hidden_size, directions * hidden_size, n_layers, dropout=self.args.decoder_layer_dropout)\n",
    "        # GRU output (seq_len, batch, directions * hidden_size)\n",
    "        self.out = nn.Linear(directions * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.input_dropout(embedded)\n",
    "        embedded = F.relu(embedded)\n",
    "        embedded = torch.unsqueeze(embedded, 0)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.out(output.squeeze(0))\n",
    "        #print(\"decoder output\", output[10,31])\n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.enc = EncoderRNN(self.args.x_dim, self.args.h_dim, n_layers=self.args.n_layers, bidirectional=args.bidirectionalEncoder, args=args)\n",
    "\n",
    "        self.dec = DecoderRNN(self.args.h_dim, self.args.output_dim, n_layers=self.args.n_layers, args=args)\n",
    "\n",
    "        self.use_schedule_sampling = args.use_schedule_sampling\n",
    "        self.scheduling_start = args.scheduling_start\n",
    "        self.scheduling_end = args.scheduling_end\n",
    "\n",
    "    def _cat_directions(self, h):\n",
    "        \"\"\" If the encoder is bidirectional, do the following transformation.\n",
    "            (#directions * #layers, #batch, hidden_size) -> (#layers, #batch, #directions * hidden_size)\n",
    "        \"\"\"\n",
    "        h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
    "        return h\n",
    "\n",
    "    def parameters(self):\n",
    "        return list(self.enc.parameters()) + list(self.dec.parameters())\n",
    "\n",
    "    def scheduleSample(self, epoch):\n",
    "        eps = max(self.args.scheduling_start - \n",
    "            (self.args.scheduling_start - self.args.scheduling_end)* epoch / self.args.n_epochs,\n",
    "            self.args.scheduling_end)\n",
    "        return np.random.binomial(1, eps)\n",
    "\n",
    "    def forward(self, x, target, epoch):\n",
    "        encoder_hidden = self.enc.initHidden()\n",
    "        hs = []\n",
    "        for t in range(self.args.input_sequence_len):\n",
    "            encoder_output, encoder_hidden = self.enc(x[t].squeeze(), encoder_hidden)\n",
    "            hs += [encoder_output]\n",
    "        if self.args.bidirectionalEncoder:\n",
    "            decoder_hidden = self._cat_directions(encoder_hidden)\n",
    "        else:\n",
    "            decoder_hidden = encoder_hidden\n",
    "        # Prepare for Decoder\n",
    "        inp = Variable(torch.zeros(self.args.batch_size, self.args.output_dim))\n",
    "        if self.args.cuda:\n",
    "            inp = inp.cuda()\n",
    "        ys = []\n",
    "        if not self.training:\n",
    "            sample=0\n",
    "        else:\n",
    "            sample = self.scheduleSample(epoch)\n",
    "        # Decode\n",
    "        for t in range(self.args.target_sequence_len):\n",
    "            decoder_output, decoder_hidden = self.dec(inp, decoder_hidden)\n",
    "            if sample:\n",
    "                inp = target[t-1]\n",
    "            else:\n",
    "                inp = decoder_output\n",
    "            ys += [decoder_output]\n",
    "        return torch.cat([torch.unsqueeze(y, dim=0) for y in ys])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/SketchRNN.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SketchRNNEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SketchRNNEncoder, self).__init__()\n",
    "        self.args = args\n",
    "        if self.args.bidirectionalEncoder:\n",
    "            self.directions = 2\n",
    "        else:\n",
    "            self.directions = 1\n",
    "        # bidirectional lstm:\n",
    "        self.lstm = nn.LSTM(self.args.x_dim * self.args.channels, self.args.encoder_h_dim, \\\n",
    "            self.args.n_layers, dropout=self.args.encoder_layer_dropout, bidirectional=self.args.bidirectionalEncoder)\n",
    "        # create mu and sigma from lstm's last output:\n",
    "        self.fc_mu = nn.Linear(self.args.n_layers * self.directions * self.args.encoder_h_dim, self.args.z_dim)\n",
    "        self.fc_sigma = nn.Linear(self.args.n_layers * self.directions * self.args.encoder_h_dim, self.args.z_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            hidden_cell = self.init_hidden_cell()\n",
    "        _, (hidden, cell) = self.lstm(input, hidden_cell)\n",
    "        # convert hidden size from (n_layers * directions, batch_size, h_dim)\n",
    "        #                       to (batch_size, n_layers * directions * h_dim)\n",
    "        hiddenLayers = torch.split(hidden, 1, 0)\n",
    "        if self.directions == 2 and self.args.n_layers == 2:\n",
    "            assert len(hiddenLayers) == 4\n",
    "        hidden_cat = torch.cat([h.squeeze(0) for h in hiddenLayers], 1)\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "        sigma_hat = self.fc_sigma(hidden_cat)\n",
    "        sigma = torch.exp(sigma_hat / 2)\n",
    "        z_size = mu.size()\n",
    "        if self.args.cuda:\n",
    "            N = Variable(torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda())\n",
    "        else:\n",
    "            N = Variable(torch.normal(torch.zeros(z_size),torch.ones(z_size)))\n",
    "        z = mu + sigma*N\n",
    "        return z, mu, sigma_hat\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden_cell(self):\n",
    "        hidden = Variable(torch.zeros(self.directions * self.args.n_layers, self.args.batch_size, self.args.encoder_h_dim))\n",
    "        cell = Variable(torch.zeros(self.directions * self.args.n_layers, self.args.batch_size, self.args.encoder_h_dim))\n",
    "        if self.args.cuda:\n",
    "            return (hidden.cuda(), cell.cuda())\n",
    "        else:\n",
    "            return (hidden, cell)\n",
    "\n",
    "class SketchRNNDecoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SketchRNNDecoder, self).__init__()\n",
    "        self.args = args\n",
    "        # to init hidden and cell from z:\n",
    "        self.fc_hc = nn.Linear(self.args.z_dim, 2 * self.args.n_layers * self.args.decoder_h_dim)\n",
    "        # unidirectional lstm:\n",
    "        self.lstm = nn.LSTM(self.args.z_dim + self.args.output_dim, self.args.decoder_h_dim, self.args.n_layers, dropout=self.args.decoder_layer_dropout)\n",
    "        self.muLayer = nn.Linear(self.args.decoder_h_dim, self.args.output_dim * self.args.n_gaussians)\n",
    "        self.sigmaLayer = nn.Linear(self.args.decoder_h_dim, self.args.output_dim * self.args.n_gaussians)\n",
    "        self.piLayer = nn.Linear(self.args.decoder_h_dim, self.args.output_dim * self.args.n_gaussians)\n",
    "\n",
    "    def forward(self, inputs, z, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            layers = torch.split(torch.tanh(self.fc_hc(z)),self.args.decoder_h_dim,1)\n",
    "            hidden = torch.stack(layers[:int(len(layers) / 2)], dim=0)\n",
    "            cell = torch.stack(layers[int(len(layers) / 2): ], dim=0)\n",
    "            hidden_cell = (hidden.contiguous(), cell.contiguous())\n",
    "        outputs,(hidden,cell) = self.lstm(inputs, hidden_cell)\n",
    "        # outputs size: (seq_len, batch, num_directions * hidden_size)\n",
    "        # hidden size: (num_layers * num_directions, batch, hidden_size)\n",
    "        # cell size: (num_layers * num_directions, batch, hidden_size)\n",
    "        mu = self.muLayer(outputs).view(-1, self.args.batch_size, self.args.output_dim, self.args.n_gaussians)\n",
    "        sigma = self.sigmaLayer(outputs).view(-1, self.args.batch_size, self.args.output_dim, self.args.n_gaussians)\n",
    "        pi = self.piLayer(outputs).view(-1, self.args.batch_size, self.args.output_dim, self.args.n_gaussians)\n",
    "        pi = F.softmax(pi, 3)\n",
    "        sigma = torch.exp(sigma)\n",
    "        return (pi, mu, sigma), (hidden, cell)\n",
    "\n",
    "class SketchRNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SketchRNN, self).__init__()\n",
    "        self.args = args\n",
    "        if self.args.cuda:\n",
    "            self.encoder = SketchRNNEncoder(args).cuda()\n",
    "            self.decoder = SketchRNNDecoder(args).cuda()\n",
    "        else:\n",
    "            self.encoder = SketchRNNEncoder(args)\n",
    "            self.decoder = SketchRNNDecoder(args)\n",
    "\n",
    "    def scheduleSample(self, epoch):\n",
    "        eps = max(self.args.scheduling_start - \n",
    "            (self.args.scheduling_start - self.args.scheduling_end)* epoch / self.args.self.args.n_epochs,\n",
    "            self.args.scheduling_end)\n",
    "        return np.random.binomial(1, eps)\n",
    "\n",
    "    def generatePred(self, pi, mu, sigma, individual=False):\n",
    "        if self.args.cuda:\n",
    "            N = torch.randn(pi.size()).cuda()\n",
    "            #N = torch.normal(torch.zeros(pi.size()),torch.ones(pi.size()))\n",
    "        else:\n",
    "            N = torch.randn(pi.size())\n",
    "            #N = torch.normal(torch.zeros(pi.size()),torch.ones(pi.size()))\n",
    "        clusterPredictions = mu + sigma * N\n",
    "        weightedClusterPredictions = clusterPredictions * pi\n",
    "        if individual:\n",
    "            pred = torch.sum(weightedClusterPredictions, dim=1)\n",
    "        else:\n",
    "            pred = torch.sum(weightedClusterPredictions, dim=3)\n",
    "        return pred\n",
    "\n",
    "    def allSteps(self, target, z):\n",
    "        sos = self.getStartOfSequence()\n",
    "        batch_init = torch.cat([sos, target[:-1,...]], 0)\n",
    "        z_stack = torch.stack([z]*(self.args.target_sequence_len))\n",
    "        inp = torch.cat([batch_init, z_stack], 2)\n",
    "        (pi, mu, sigma), (hidden, cell) = self.decoder(inp, z)\n",
    "        return (pi, mu, sigma)\n",
    "\n",
    "    def oneStepAtATime(self, z):\n",
    "        sos = self.getStartOfSequence()\n",
    "        inp = torch.cat([sos, z.unsqueeze(0)], 2)\n",
    "        piList, muList, sigmaList = [], [], []\n",
    "        for timeStep in range(self.args.target_sequence_len):\n",
    "            (pi, mu, sigma), (hidden, cell) = self.decoder(inp, z)\n",
    "            pred = self.generatePred(pi, mu, sigma)\n",
    "            inp = torch.cat([pred, z.unsqueeze(0)], 2)\n",
    "            piList.append(pi.detach())\n",
    "            muList.append(mu.detach())\n",
    "            sigmaList.append(sigma.detach())\n",
    "        Pi = torch.cat(piList, 0)\n",
    "        Mu = torch.cat(muList, 0)\n",
    "        Sigma = torch.cat(sigmaList, 0)\n",
    "        return (Pi, Mu, Sigma)\n",
    "\n",
    "    def getStartOfSequence(self):\n",
    "        if self.args.cuda:\n",
    "            return Variable(torch.zeros(1, self.args.batch_size, self.args.output_dim).cuda())\n",
    "        else:\n",
    "            return Variable(torch.zeros(1, self.args.batch_size, self.args.output_dim))\n",
    "\n",
    "    def doEncoding(self,batch):\n",
    "        # convert input from [input_sequence_len, batch_size, channels, x_dim]\n",
    "        #                 to [input_sequence_len, batch_size, channels * x_dim]\n",
    "        embedded = batch.contiguous().view(-1, self.args.batch_size, self.args.x_dim * self.args.channels)\n",
    "        z, mu, sigma_hat = self.encoder(embedded)\n",
    "        return z, mu, sigma_hat\n",
    "\n",
    "    def forward(self, batch, target, epoch):\n",
    "        z, latentMean, latentStd = self.doEncoding(batch)\n",
    "        if self.training:\n",
    "            (Pi, Mu, Sigma) = self.allSteps(target, z)\n",
    "        else:\n",
    "            (Pi, Mu, Sigma) = self.oneStepAtATime(z)\n",
    "        return Pi, Mu, Sigma, latentMean, latentStd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/encoders.py\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class RecurrentEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, input_dim, hidden_dim,\n",
    "                 dropout_prob, bidirectional, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.embedding_layer = nn.Linear(in_features=self.input_dim,\n",
    "                                           out_features=self.input_dim)\n",
    "        if num_layers == 1:\n",
    "            RNNDROPOUT = 0.0\n",
    "        else:\n",
    "            RNNDROPOUT = dropout_prob\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=input_dim, hidden_size=hidden_dim,\n",
    "                bidirectional=bidirectional, num_layers=num_layers,\n",
    "                dropout=RNNDROPOUT)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=input_dim, hidden_size=hidden_dim,\n",
    "                bidirectional=bidirectional, num_layers=num_layers,\n",
    "                dropout=RNNDROPOUT)\n",
    "        else:\n",
    "            raise ValueError('Unknown RNN type!')\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.normal_(self.embedding_layer.weight.data, mean=0, std=0.01)\n",
    "        for i in range(self.num_layers):\n",
    "            suffixes = ['']\n",
    "            if self.bidirectional:\n",
    "                suffixes.append('_reverse')\n",
    "            for suffix in suffixes:\n",
    "                weight_ih = getattr(self.rnn, f'weight_ih_l{i}{suffix}')\n",
    "                weight_hh = getattr(self.rnn, f'weight_hh_l{i}{suffix}')\n",
    "                bias_ih = getattr(self.rnn, f'bias_ih_l{i}{suffix}')\n",
    "                bias_hh = getattr(self.rnn, f'bias_hh_l{i}{suffix}')\n",
    "                init.orthogonal_(weight_hh.data)\n",
    "                init.kaiming_normal_(weight_ih.data)\n",
    "                init.constant_(bias_ih.data, val=0)\n",
    "                init.constant_(bias_hh.data, val=0)\n",
    "                if self.rnn_type == 'lstm':  # Set initial forget bias to 1\n",
    "                    bias_ih.data.chunk(4)[1].fill_(1)\n",
    "\n",
    "    def forward(self, encoder_inputs):\n",
    "        encoder_inputs_emb = self.embedding_layer(encoder_inputs)\n",
    "        encoder_inputs_emb = self.dropout(encoder_inputs_emb)\n",
    "        encoder_hidden_states, rnn_state = self.rnn(encoder_inputs_emb)\n",
    "        # For LSTM, encoder_states does not contain cell states.\n",
    "        # Thus it is necessary to explicitly return the last state\n",
    "        encoder_state = rnn_state\n",
    "        return encoder_hidden_states, encoder_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/basic.py\n",
    "import torch\n",
    "from torch.nn import functional\n",
    "\n",
    "\n",
    "def sequence_mask(length, max_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        length (Tensor): A long tensor of size (batch_size,).\n",
    "        max_length (int): The maximum length. If None, it automatically\n",
    "            sets this as max(lengths).\n",
    "    Returns:\n",
    "        mask: (Tensor): A byte mask tensor of size\n",
    "            (batch_size, max_length). Each element is 1 if valid\n",
    "            and 0 else.\n",
    "    \"\"\"\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = length.max()\n",
    "    seq_range = torch.arange(0, max_length).unsqueeze(0).type_as(length)\n",
    "    length = length.unsqueeze(1)\n",
    "    mask = torch.lt(seq_range, length)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def sequence_cross_entropy(logits, targets, length):\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    targets_flat = targets.view(-1)\n",
    "    losses_flat = functional.cross_entropy(\n",
    "        input=logits_flat, target=targets_flat, reduce=False)\n",
    "    losses = losses_flat.view(*targets.size())\n",
    "    mask = sequence_mask(length=length, max_length=logits.size(0)).t()\n",
    "    losses.data.masked_fill_(mask=~mask, value=0)\n",
    "    loss = losses.sum() / losses.size(1)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/attention.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init, functional\n",
    "\n",
    "# from . import basic\n",
    "\n",
    "\n",
    "class DotAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.attend = nn.Sequential(nn.Linear(in_features=2 * hidden_dim,\n",
    "            out_features=hidden_dim),\n",
    "            nn.Tanh())\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_normal_(self.attend[0].weight.data)\n",
    "        init.constant_(self.attend[0].bias.data, val=0)\n",
    "\n",
    "    def score(self, queries, annotations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries (Variable): Query vectors to annotations,\n",
    "                (batch_size, tgt_len, hidden_dim).\n",
    "            annotations (Variable): Encoded source vectors,\n",
    "                (batch_size, src_len, hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            scores (Variable): Alignment vectors,\n",
    "                (batch_size, tgt_len, src_len).\n",
    "        \"\"\"\n",
    "\n",
    "        scores = torch.bmm(queries, annotations.transpose(1, 2))\n",
    "        scores = functional.softmax(scores, dim=2)\n",
    "        return scores\n",
    "\n",
    "    def forward(self, queries, annotations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries (Variable): Query vectors to annotations,\n",
    "                (tgt_len, batch_size, hidden_dim).\n",
    "            annotations (Variable): Encoded source vectors,\n",
    "                (src_len, batch_size, hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            attentional_states (Variable): Hidden states after\n",
    "                being attended, (tgt_len, batch_size, hidden_dim).\n",
    "            scores (Variable): Alignment vectors,\n",
    "                (tgt_len, batch_size, src_len).\n",
    "        \"\"\"\n",
    "\n",
    "        # Make queries and annotations batch-major.\n",
    "        queries = queries.permute(1,0,2)\n",
    "        annotations = annotations.permute(1,0,2)\n",
    "        scores = self.score(queries=queries, annotations=annotations)\n",
    "        contexts = torch.bmm(scores, annotations)\n",
    "        attention_features = [queries, contexts]\n",
    "        attention_input = torch.cat(attention_features, dim=2)\n",
    "        attention_input = self.dropout(attention_input)\n",
    "        attentional_states = self.attend(attention_input)\n",
    "\n",
    "        # Convert back to time-major.\n",
    "        attentional_states = attentional_states.permute(1,0,2)\n",
    "        scores = scores.permute(1,0,2)\n",
    "        return attentional_states, scores\n",
    "\n",
    "\n",
    "class MLPAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, annotation_dim, dropout_prob):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.query_linear = nn.Linear(in_features=hidden_dim,\n",
    "                                      out_features=hidden_dim)\n",
    "        self.annotation_linear = nn.Linear(in_features=annotation_dim,\n",
    "                                           out_features=hidden_dim,\n",
    "                                           bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.v = nn.Linear(in_features=hidden_dim, out_features=1,\n",
    "                           bias=False)\n",
    "        self.attend = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_dim + annotation_dim,\n",
    "                      out_features=hidden_dim),\n",
    "            nn.Tanh())\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.normal_(self.query_linear.weight.data, mean=0, std=0.001)\n",
    "        init.constant_(self.query_linear.bias.data, val=0)\n",
    "        init.normal_(self.annotation_linear.weight.data, mean=0, std=0.001)\n",
    "        init.constant_(self.v.weight.data, val=0)\n",
    "        init.kaiming_normal_(self.attend[0].weight.data)\n",
    "        init.constant_(self.attend[0].bias.data, val=0)\n",
    "\n",
    "    def score(self, queries, annotations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries (Variable): Query vectors to annotations,\n",
    "                (batch_size, tgt_len, hidden_dim).\n",
    "            annotations (Variable): Encoded source vectors,\n",
    "                (batch_size, src_len, annotation_dim).\n",
    "\n",
    "        Returns:\n",
    "            scores (Variable): Alignment vectors,\n",
    "                (batch_size, tgt_len, src_len).\n",
    "        \"\"\"\n",
    "\n",
    "        # queries_proj: (batch_size, tgt_len, hidden_dim)\n",
    "        queries_proj = self.query_linear(queries)\n",
    "        # annotations_proj: (batch_size, src_len, hidden_dim)\n",
    "        annotations_proj = self.annotation_linear(annotations)\n",
    "        pre_tanh = queries_proj.unsqueeze(2) + annotations_proj.unsqueeze(1)\n",
    "        scores = self.v(self.tanh(pre_tanh)).squeeze(3)\n",
    "        scores = functional.softmax(scores, dim=2)\n",
    "        return scores\n",
    "\n",
    "    def forward(self, queries, annotations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            queries (Variable): Query vectors to annotations,\n",
    "                (tgt_len, batch_size, hidden_dim).\n",
    "            annotations (Variable): Encoded source vectors,\n",
    "                (src_len, batch_size, hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            attentional_states (Variable): Hidden states after\n",
    "                being attended, (tgt_len, batch_size, hidden_dim).\n",
    "            scores (Variable): Alignment vectors,\n",
    "                (tgt_len, batch_size, src_len).\n",
    "        \"\"\"\n",
    "\n",
    "        # Make queries and annotations batch-major.\n",
    "        queries = queries.permute(1,0,2)\n",
    "        annotations = annotations.permute(1,0,2)\n",
    "        scores = self.score(queries=queries, annotations=annotations)\n",
    "        contexts = torch.bmm(scores, annotations)\n",
    "        attention_features = [queries, contexts]\n",
    "        attention_input = torch.cat(attention_features, dim=2)\n",
    "        attention_input = self.dropout(attention_input)\n",
    "        attentional_states = self.attend(attention_input)\n",
    "\n",
    "        # Convert back to time-major.\n",
    "        attentional_states = attentional_states.permute(1,0,2)\n",
    "        scores = scores.permute(1,0,2)\n",
    "        return attentional_states, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/decoders.py\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RecurrentDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, input_dim, hidden_dim,\n",
    "                 annotation_dim, num_layers, attention_type, input_feeding,\n",
    "                 dropout_prob, args):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.annotation_dim = annotation_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.input_feeding = input_feeding\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.args = args\n",
    "        self.attention_type = attention_type\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.word_embedding = nn.Linear(in_features=self.input_dim,\n",
    "                                        out_features=self.input_dim)\n",
    "        rnn_input_size = input_dim\n",
    "        if input_feeding and attention_type != \"None\":\n",
    "            rnn_input_size += hidden_dim\n",
    "        if num_layers == 1:\n",
    "            RNNDROPOUT = 0.0\n",
    "        else:\n",
    "            RNNDROPOUT = self.dropout_prob\n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=rnn_input_size, hidden_size=hidden_dim,\n",
    "                num_layers=num_layers, dropout=RNNDROPOUT)\n",
    "        elif rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=rnn_input_size, hidden_size=hidden_dim,\n",
    "                num_layers=num_layers, dropout=RNNDROPOUT)\n",
    "        else:\n",
    "            raise ValueError('Unknown RNN type!')\n",
    "        if attention_type == 'dot':\n",
    "            assert hidden_dim == annotation_dim, (\n",
    "                'hidden_dim and annotation_dim must be same when using'\n",
    "                ' dot attention.')\n",
    "            self.attention = DotAttention(\n",
    "                hidden_dim=hidden_dim, dropout_prob=dropout_prob)\n",
    "        elif attention_type == 'mlp':\n",
    "            self.attention = MLPAttention(\n",
    "                hidden_dim=hidden_dim, annotation_dim=annotation_dim,\n",
    "                dropout_prob=dropout_prob)\n",
    "        elif attention_type == \"None\":\n",
    "            self.attention = nn.Sequential(nn.Linear(in_features=hidden_dim,\n",
    "            out_features=hidden_dim),\n",
    "            nn.Tanh())\n",
    "        else:\n",
    "            raise ValueError('Unknown attention type!')\n",
    "        self.output_linear = nn.Linear(in_features=hidden_dim,\n",
    "                                       out_features=input_dim)\n",
    "        self.reset_parameters()\n",
    "        self.resizeEncoderState = nn.Linear(in_features=hidden_dim * 2, out_features=hidden_dim)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.normal_(self.word_embedding.weight.data, mean=0, std=0.01)\n",
    "        for i in range(self.num_layers):\n",
    "            weight_ih = getattr(self.rnn, f'weight_ih_l{i}')\n",
    "            weight_hh = getattr(self.rnn, f'weight_hh_l{i}')\n",
    "            bias_ih = getattr(self.rnn, f'bias_ih_l{i}')\n",
    "            bias_hh = getattr(self.rnn, f'bias_hh_l{i}')\n",
    "            init.orthogonal_(weight_hh.data)\n",
    "            init.kaiming_normal_(weight_ih.data)\n",
    "            init.constant_(bias_ih.data, val=0)\n",
    "            init.constant_(bias_hh.data, val=0)\n",
    "            if self.rnn_type == 'lstm':  # Set initial forget bias to 1\n",
    "                bias_ih.data.chunk(4)[1].fill_(1)\n",
    "        if self.attention_type != \"None\":\n",
    "            self.attention.reset_parameters()\n",
    "        init.normal_(self.output_linear.weight.data, mean=0, std=0.01)\n",
    "        init.constant_(self.output_linear.bias.data, val=0)\n",
    "\n",
    "    def scheduleSample(self, epoch):\n",
    "        eps = max(self.args.scheduling_start - \n",
    "            (self.args.scheduling_start - self.args.scheduling_end)* epoch / self.args.n_epochs,\n",
    "            self.args.scheduling_end)\n",
    "        return np.random.binomial(1, eps)\n",
    "\n",
    "    def forward(self, annotations, targets, state, epoch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotations (Variable): A float variable of size\n",
    "                (max_src_length, batch_size, context_dim).\n",
    "            decoder_inputs (Variable): A long variable of size\n",
    "                (seq_length, batch_size, n_features) that either the previous\n",
    "                target or prediction.\n",
    "            state (DecoderState): The current state of the decoder.\n",
    "\n",
    "        Returns:\n",
    "            logits (Variable): A float variable containing unnormalized\n",
    "                log probabilities. It has the same size as words.\n",
    "            state (DecoderState): The updated state of the decoder.\n",
    "            attention_weights (Variable): A float variable of size\n",
    "                (length, batch_size, max_src_length), which contains\n",
    "                the attention weight for each time step of the context.\n",
    "        \"\"\"\n",
    "        inp = torch.zeros(targets[0].size())\n",
    "        if self.args.cuda:\n",
    "            inp = inp.cuda()\n",
    "        decoder_inputs = torch.cat([inp.unsqueeze(0), targets[:-1]], dim=0)\n",
    "        state = copy.copy(state)\n",
    "        bidirectionalEncoder = state.prepForDecoder(num_layers=self.num_layers)\n",
    "        if bidirectionalEncoder:\n",
    "            if isinstance(state.rnn, tuple):\n",
    "                hidden = self.resizeEncoderState(state.rnn[0])\n",
    "                cell = self.resizeEncoderState(state.rnn[1])\n",
    "                state.update_rnn_state((hidden, cell))\n",
    "            else:\n",
    "                state.update_rnn_state(self.resizeEncoderState(state.rnn))\n",
    "        if not self.input_feeding:\n",
    "            embedded_inputs = self.word_embedding(decoder_inputs)\n",
    "            embedded_inputs = self.dropout(embedded_inputs)\n",
    "            rnn_outputs, rnn_state = self.rnn(input=embedded_inputs, hx=state.rnn)\n",
    "            if self.attention_type != \"None\":\n",
    "                attentional_states, attention_weights = self.attention(\n",
    "                    queries=rnn_outputs, annotations=annotations)\n",
    "            else:\n",
    "                attentional_states = rnn_outputs\n",
    "                attention_weights = torch.zeros((1,1))\n",
    "                if self.args.cuda:\n",
    "                    attention_weights = attention_weights.cuda()\n",
    "            state.update(rnn_state=rnn_state)\n",
    "            attentional_states = self.output_linear(attentional_states)\n",
    "        else:\n",
    "            target_seq_len, batch_size, _ = decoder_inputs.size()\n",
    "            attentional_states = []\n",
    "            attention_weights = []\n",
    "            if state.attention is None:\n",
    "                zero_attentional_state = torch.zeros((batch_size, self.hidden_dim))\n",
    "                if self.args.cuda:\n",
    "                    zero_attentional_state = zero_attentional_state.cuda()\n",
    "                # zero_attentional_state = (\n",
    "                #     embedded_inputs.data.new(batch_size, self.hidden_dim).zero_())\n",
    "                zero_attentional_state = Variable(zero_attentional_state)\n",
    "                state.update_attentional_state(zero_attentional_state)\n",
    "            if self.args.no_schedule_sampling or not self.training:\n",
    "                sample=0\n",
    "            else:\n",
    "                sample = self.scheduleSample(epoch)\n",
    "            for t in range(target_seq_len):\n",
    "                inp = self.word_embedding(inp)\n",
    "                inp = self.dropout(inp)\n",
    "                if self.attention_type != \"None\":\n",
    "                    decoder_input_t = torch.cat([inp, state.attention], dim=1)\n",
    "                    decoder_input_t = decoder_input_t.unsqueeze(0)\n",
    "                    rnn_output_t, rnn_state_t = self.rnn(\n",
    "                        input=decoder_input_t, hx=state.rnn)\n",
    "                    attentional_state_t, attention_weights_t = self.attention(\n",
    "                        queries=rnn_output_t, annotations=annotations)\n",
    "                else:\n",
    "                    attentional_state_t, rnn_state_t = self.rnn(\n",
    "                        input=inp.unsqueeze(0), hx=state.rnn)\n",
    "                    attention_weights_t = torch.zeros((1,1))\n",
    "                    if self.args.cuda:\n",
    "                        attention_weights_t = attention_weights_t.cuda()\n",
    "                attentional_state_t = attentional_state_t.squeeze(0)\n",
    "                state.update_state(rnn_state=rnn_state_t,\n",
    "                                   attentional_state=attentional_state_t)\n",
    "                attentional_state_t = self.output_linear(attentional_state_t)\n",
    "                attentional_states.append(attentional_state_t)\n",
    "                attention_weights.append(attention_weights_t)\n",
    "                \n",
    "                if sample:\n",
    "                    inp = targets[t].squeeze()\n",
    "                else:\n",
    "                    inp = attentional_state_t.squeeze()\n",
    "            attentional_states = torch.stack(attentional_states, dim=0)\n",
    "            attention_weights = torch.cat(attention_weights, dim=0)\n",
    "        logits = attentional_states\n",
    "        return logits, state, attention_weights\n",
    "\n",
    "\n",
    "class DecoderState(dict):\n",
    "\n",
    "    def __init__(self, rnn_state, attentional_state=None,\n",
    "                 input_feeding=False, decoder_layers=None):\n",
    "        super().__init__()\n",
    "        self['input_feeding'] = input_feeding\n",
    "        self['rnn'] = rnn_state\n",
    "        assert input_feeding or attentional_state is None\n",
    "        self['attention'] = None\n",
    "        if input_feeding:\n",
    "            self['attention'] = attentional_state\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_to_rnn_state(fn, rnn_state):\n",
    "        if isinstance(rnn_state, tuple):  # LSTM\n",
    "            return tuple(fn(s) for s in rnn_state)\n",
    "        else:\n",
    "            return fn(rnn_state)\n",
    "\n",
    "    @property\n",
    "    def input_feeding(self):\n",
    "        return self['input_feeding']\n",
    "\n",
    "    @property\n",
    "    def rnn(self):\n",
    "        return self['rnn']\n",
    "\n",
    "    @property\n",
    "    def attention(self):\n",
    "        return self['attention']\n",
    "\n",
    "    def prepForDecoder(self, num_layers):\n",
    "        if isinstance(self[\"rnn\"], tuple):\n",
    "            # LSTM\n",
    "            newState = []\n",
    "            for i in range(2):\n",
    "                numLayersTDirections, batch_size, hidden_dim = self[\"rnn\"][i].size()\n",
    "                separateDirections = self[\"rnn\"][i].view(num_layers, -1, batch_size, hidden_dim)\n",
    "                if separateDirections.size(1) == 2:\n",
    "                    forwardHidden = separateDirections[:,0,:,:]\n",
    "                    reverseHidden = separateDirections[:,1,:,:]\n",
    "                    hidden = torch.cat([forwardHidden, reverseHidden], dim=2)\n",
    "                else:\n",
    "                    hidden = separateDirections[:,0,:,:]\n",
    "                newState.append(hidden)\n",
    "            self[\"rnn\"] = (newState[0], newState[1])\n",
    "        else:\n",
    "            # GRU\n",
    "            numLayersTDirections, batch_size, hidden_dim = self[\"rnn\"].size()\n",
    "            separateDirections = self[\"rnn\"].view(num_layers, -1, batch_size, hidden_dim)\n",
    "            if separateDirections.size(1) == 2:\n",
    "                forwardHidden = separateDirections[:,0,:,:]\n",
    "                reverseHidden = separateDirections[:,1,:,:]\n",
    "                hidden = torch.cat([forwardHidden, reverseHidden], dim=2)\n",
    "            else:\n",
    "                hidden = separateDirections[:,0,:,:]\n",
    "            self[\"rnn\"] = hidden\n",
    "        return separateDirections.size(1) == 2\n",
    "\n",
    "\n",
    "\n",
    "    def update_state(self, rnn_state=None, attentional_state=None):\n",
    "        self.update_rnn_state(rnn_state)\n",
    "        self.update_attentional_state(attentional_state)\n",
    "\n",
    "    def update_rnn_state(self, rnn_state):\n",
    "        self['rnn'] = rnn_state\n",
    "\n",
    "    def update_attentional_state(self, attentional_state):\n",
    "        if self.input_feeding and attentional_state is not None:\n",
    "            self['attention'] = attentional_state\n",
    "\n",
    "    def repeat(self, beam_size):\n",
    "        new_state = copy.copy(self)\n",
    "        new_state['rnn'] = self.apply_to_rnn_state(\n",
    "            fn=lambda s: s.repeat(1, beam_size, 1),\n",
    "            rnn_state=new_state['rnn'])\n",
    "        if self.input_feeding and new_state['attention'] is not None:\n",
    "            new_state['attention'] = (\n",
    "                new_state['attention'].repeat(beam_size, 1))\n",
    "        return new_state\n",
    "\n",
    "    def beam_update(self, batch_index, beam_indices, beam_size):\n",
    "        def beam_update_fn(v):\n",
    "            # The shape is (..., beam_size * batch_size, state_dim).\n",
    "            orig_size = v.size()\n",
    "            new_size = (\n",
    "                orig_size[:-2]\n",
    "                + (beam_size, orig_size[-2] // beam_size, orig_size[-1]))\n",
    "            # beam_of_batch: (..., beam_size, state_dim)\n",
    "            beam_of_batch = v.view(*new_size).select(-2, batch_index)\n",
    "            beam_of_batch.data.copy_(\n",
    "                beam_of_batch.data.index_select(-2, beam_indices))\n",
    "\n",
    "        self.apply_to_rnn_state(fn=beam_update_fn, rnn_state=self['rnn'])\n",
    "        if self.input_feeding and self['attention'] is not None:\n",
    "            beam_update_fn(self['attention'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# %load ../batchedRNN/model/seq2seq.py\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from . import encoders, decoders\n",
    "\n",
    "\n",
    "class RecurrentSeq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, dropout_prob,\n",
    "                rnn_type, bidirectional, num_layers,\n",
    "                attention_type, input_feeding,\n",
    "                input_dim, output_dim, args):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.rnn_type = rnn_type\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.attention_type = dropout_prob\n",
    "        self.input_feeding = input_feeding\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.encoder = RecurrentEncoder(\n",
    "            rnn_type=rnn_type, input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim, dropout_prob=dropout_prob,\n",
    "            bidirectional=bidirectional, num_layers=num_layers)\n",
    "        annotation_dim = hidden_dim\n",
    "        if bidirectional:\n",
    "            annotation_dim = hidden_dim * 2\n",
    "        self.decoder = RecurrentDecoder(\n",
    "            rnn_type=rnn_type, input_dim=output_dim, hidden_dim=hidden_dim,\n",
    "            annotation_dim=annotation_dim, num_layers=num_layers,\n",
    "            attention_type=attention_type, input_feeding=input_feeding,\n",
    "            dropout_prob=dropout_prob, args=args)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.encoder.reset_parameters()\n",
    "        self.decoder.reset_parameters()\n",
    "\n",
    "    def remove4D(self, encoder_inputs):\n",
    "        inp0, inp1 = encoder_inputs[:,:,0,:], encoder_inputs[:,:,1,:]\n",
    "        return torch.cat([inp0, inp1], dim=2)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_inputs, decoder_inputs, epoch=None):\n",
    "        encoder_hidden_states, encoder_state = self.encoder(\n",
    "            encoder_inputs=self.remove4D(encoder_inputs))\n",
    "        decoder_state = DecoderState(\n",
    "            rnn_state=encoder_state, input_feeding=self.input_feeding)\n",
    "        logits, _, _ = self.decoder(\n",
    "            annotations=encoder_hidden_states,\n",
    "            targets=decoder_inputs, state=decoder_state, epoch=epoch)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "class Bunch(object):\n",
    "    def __init__(self, adict):\n",
    "        self.__dict__.update(adict)\n",
    "    def __str__(self):\n",
    "        out = \"\"\n",
    "        for k, v in self.__dict__.items():\n",
    "            out += \"{}: {}, \".format(k, v)\n",
    "        return out\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostProcess(object):\n",
    "    def __init__(self, modelPath,args, chooseModel=\"rnn\", dataDict=None):\n",
    "        self.args = args\n",
    "        self.modelPath = modelPath\n",
    "        if dataDict:\n",
    "            self.dataDict = dataDict\n",
    "        else:\n",
    "            self.dataDict = GetDataLoader(\"../data/traffic/trafficWithTime/\")\n",
    "        if chooseModel==\"rnn\":\n",
    "            self.model = Seq2Seq(self.args)\n",
    "        elif chooseModel==\"sketch-rnn\":\n",
    "            self.model = SketchRNN(self.args)\n",
    "        elif chooseModel == \"Seq2SeqAttn\":\n",
    "            self.model = RecurrentSeq2Seq(\n",
    "                self.args.h_dim,\n",
    "                self.args.dropout_prob,\n",
    "                self.args.rnn_type,\n",
    "                self.args.bidirectionalEncoder,\n",
    "                self.args.n_layers,\n",
    "                self.args.attention_type,\n",
    "                self.args.input_feeding,\n",
    "                self.args.x_dim * args.channels,\n",
    "                self.args.output_dim,\n",
    "                self.args)\n",
    "        self.model.eval()\n",
    "\n",
    "    def getReconLoss(self, output, target):\n",
    "        output = self.dataDict[\"scaler\"].inverse_transform(output)\n",
    "        target = self.dataDict[\"scaler\"].inverse_transform(target)\n",
    "        assert output.size() == target.size(), \"output size: {}, target size: {}\".format(output.size(), target.size())\n",
    "        outputs = {}\n",
    "        \n",
    "        if args.criterion == \"RMSE\":\n",
    "            mse = nn.MSELoss()\n",
    "            loss = torch.sqrt(mse(output, target)).item()\n",
    "        elif args.criterion == \"L1Loss\":\n",
    "            l1loss = nn.L1Loss()\n",
    "            loss = l1loss(output, target).item()\n",
    "        return loss, output, target\n",
    "\n",
    "    def sketchRNNKLD(self, latentMean, latentStd, trainingMode, epoch):\n",
    "        LKL = -0.5*torch.sum(1+latentStd-latentMean**2-torch.exp(latentStd))\\\n",
    "                /float(args.z_dim*args.batch_size)\n",
    "        if trainingMode:\n",
    "            # update eta for LKL:\n",
    "            eta_step = 1-(1-args.eta_min)*args.R**epoch\n",
    "            if args.cuda:\n",
    "                KL_min = torch.Tensor([args.KL_min]).cuda()\n",
    "            else:\n",
    "                KL_min = torch.Tensor([args.KL_min])\n",
    "            return eta_step * torch.max(LKL, KL_min)\n",
    "        else:\n",
    "            return LKL\n",
    "\n",
    "    def getLoss(self, output, target, epoch):\n",
    "        if args.model == \"rnn\" or args.model == \"Seq2SeqAttn\":\n",
    "            reconLoss, pred, target = self.getReconLoss(output, target)\n",
    "            kldLoss = 0\n",
    "            addtlDict = {}\n",
    "        else:\n",
    "            Pi, Mu, Sigma, latentMean, latentStd = output\n",
    "            Pi = Pi.cpu().detach()\n",
    "            Mu = Mu.cpu().detach()\n",
    "            Sigma = Sigma.cpu().detach()\n",
    "            latentMean = latentMean.cpu().detach()\n",
    "            latentStd = latentStd.cpu().detach()\n",
    "            reconLoss = self.sketchRNNReconLoss(target, Pi, Mu, Sigma)\n",
    "            kldLoss = self.sketchRNNKLD(latentMean, latentStd, self.model.training, epoch)\n",
    "            pred = self.dataDict[\"scaler\"].inverse_transform(self.model.generatePred(Pi, Mu, Sigma))\n",
    "            target = self.dataDict[\"scaler\"].inverse_transform(target)\n",
    "            addtlDict = {\n",
    "                \"Pi\" : Pi,\n",
    "                \"Mu\" : Mu,\n",
    "                \"Sigma\": Sigma,\n",
    "                \"latentMean\" : latentMean,\n",
    "                \"latentStd\" : latentStd\n",
    "            }\n",
    "        outputDict = {\n",
    "            \"reconLoss\" : reconLoss,\n",
    "            \"kldLoss\" : kldLoss,\n",
    "            \"pred\" : pred,\n",
    "            \"target\" : target,\n",
    "            \"additional\" : addtlDict\n",
    "        }\n",
    "        return outputDict\n",
    "\n",
    "    def sketchRNNReconLoss(self, target, Pi, Mu, Sigma):\n",
    "        stackedTarget = torch.stack([target] * Mu.size(3), dim=3)\n",
    "        m = torch.distributions.Normal(loc=Mu, scale=Sigma)\n",
    "        loss = torch.exp(m.log_prob(stackedTarget))\n",
    "        loss = torch.sum(loss * Pi, dim=3)\n",
    "        loss= -torch.log(loss)\n",
    "        return loss.mean()\n",
    "\n",
    "    def getEpochLoss(self, dataset, epoch):\n",
    "        datas = []\n",
    "        preds = []\n",
    "        targets = []\n",
    "        Pi = []\n",
    "        Mu = []\n",
    "        Sigma = []\n",
    "        epoch_recon_val_loss = 0.0\n",
    "        epoch_kld_val_loss = 0.0\n",
    "        nValBatches = 0\n",
    "        with torch.no_grad():\n",
    "            for batchIDX, (inputData, target) in enumerate(map(self.dataDict[\"scaler\"].transformBatchForEpoch, self.dataDict[dataset])):\n",
    "                nValBatches += 1\n",
    "                output = self.model(inputData, target, epoch)\n",
    "                lossOutputs = self.getLoss(output, target, epoch)\n",
    "                epoch_recon_val_loss += lossOutputs[\"reconLoss\"]\n",
    "                epoch_kld_val_loss += lossOutputs[\"kldLoss\"]\n",
    "                preds.append(lossOutputs[\"pred\"].cpu().detach().numpy())\n",
    "                targets.append(lossOutputs[\"target\"].cpu().detach().numpy())\n",
    "                datas.append(self.dataDict[\"scaler\"].inverse_transform(inputData[:,:,0,:]).cpu().detach().numpy())\n",
    "                if args.model == \"sketch-rnn\":\n",
    "                    Pi.append(lossOutputs[\"additional\"][\"Pi\"])\n",
    "                    Mu.append(lossOutputs[\"additional\"][\"Mu\"])\n",
    "                    Sigma.append(lossOutputs[\"additional\"][\"Sigma\"])\n",
    "                \n",
    "        datas = np.concatenate(datas, axis=0)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        targets = np.concatenate(targets, axis=0)\n",
    "        if args.model == \"sketch-rnn\":\n",
    "            Pi = np.concatenate(Pi, axis=1)\n",
    "            Mu = np.concatenate(Mu, axis=1)\n",
    "            Sigma = np.concatenate(Sigma, axis=1)\n",
    "        retVals = {\n",
    "            \"reconLoss\" : epoch_recon_val_loss / nValBatches,\n",
    "            \"kldLoss\" : epoch_kld_val_loss / nValBatches,\n",
    "            \"preds\" : preds,\n",
    "            \"targets\" : targets,\n",
    "            \"datas\" : datas,\n",
    "            \"Pi\" : Pi,\n",
    "            \"Mu\" : Mu,\n",
    "            \"Sigma\" : Sigma\n",
    "        }\n",
    "        return retVals\n",
    "\n",
    "    def prep(self, stateDictFile, dataset, epoch):\n",
    "        desired_state_dict = torch.load(self.modelPath +stateDictFile, map_location=lambda storage, loc: storage)\n",
    "        self.model.load_state_dict(desired_state_dict)\n",
    "        self.model.eval()\n",
    "        assert dataset in [\"train\", \"val\", \"test\"]\n",
    "        \n",
    "\n",
    "    def getLossAtEpoch(self, stateDictFile, dataset, epoch= -1):\n",
    "        self.prep(stateDictFile, dataset, epoch)\n",
    "        retVals = self.getEpochLoss(dataset, epoch)\n",
    "        return retVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plotTrainValCurve(trainLosses, valLosses, trainKLDLosses=None, valKLDLosses=None, labelTotal=False):\n",
    "    plot_every = 1\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    plt.figure()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    if args.model == \"sketch-rnn\":\n",
    "        if labelTotal:\n",
    "            ax1.set_ylabel(\"NLL + KLD Loss\", color=\"r\")\n",
    "        else:\n",
    "            ax1.set_ylabel(\"NLL\", color=\"r\")\n",
    "    else:\n",
    "        ax1.set_ylabel(args.criterion)\n",
    "    ax1.tick_params('y')\n",
    "    if labelTotal:\n",
    "        labelT = \"total train loss\"\n",
    "        labelV = \"total validation loss\"\n",
    "    else:\n",
    "        labelT = \"train reconstruction loss\"\n",
    "        labelV = \"validation reconstruction loss\"\n",
    "    ax1.plot(np.arange(1, len(trainLosses)+1)*plot_every, trainLosses, \"r--\", label=labelT)\n",
    "    ax1.plot(np.arange(1, len(valLosses)+1)*plot_every, valLosses, color=\"red\", label=labelV)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.grid()\n",
    "    if trainKLDLosses:\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel(\"KLD Loss\", color=\"b\")\n",
    "        ax2.tick_params('y', colors='b')\n",
    "        ax2.plot(np.arange(1, len(trainKLDLosses)+1)*plot_every, trainKLDLosses, \"b--\", label=\"train KLD loss\")\n",
    "        ax2.plot(np.arange(1, len(valKLDLosses)+1)*plot_every, valKLDLosses, color=\"blue\", label=\"val KLD loss\")\n",
    "        ax2.legend(loc=\"upper right\")\n",
    "        ax2.grid()\n",
    "    plt.title(\"Losses for {}\".format(args.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForPreds(dataFile, batch_size, nHours=24):\n",
    "    f = np.load(dataFile)\n",
    "    startRow = np.random.choice(range(0, f[\"inputTimes\"].shape[0] - 12*nHours))\n",
    "    randomSensor = np.random.choice(range(0, f[\"inputs\"].shape[2]))\n",
    "    inputs = f[\"inputs\"][startRow:startRow+12*nHours:12]\n",
    "    targets = f[\"targets\"][startRow:startRow+12*nHours:12]\n",
    "    inputTimes = f[\"inputTimes\"][startRow:startRow+12*nHours:12].reshape(-1,1)\n",
    "    targetTimes = f[\"targetTimes\"][startRow:startRow+12*nHours:12].reshape(-1,1)\n",
    "    return inputs, targets, inputTimes, targetTimes, randomSensor\n",
    "\n",
    "def makePreds(dataDict, model, batch_size, randomSensor, inputs, targets):\n",
    "    nBatches = int(np.ceil(inputs.shape[0] / batch_size))\n",
    "    datas, targetVals, preds = [],[],[]\n",
    "    for batch in range(nBatches):\n",
    "        if batch * (1 + batch_size) < inputs.shape[0]:\n",
    "            inp = torch.FloatTensor(inputs[batch * batch_size : (1 + batch) * batch_size])\n",
    "            target = torch.FloatTensor(targets[batch * batch_size : batch_size * (1 + batch)])\n",
    "            stripLast = 0\n",
    "        else:\n",
    "            stripLast = 10 - inp_partial.shape[0]\n",
    "            inp_partial = inputs[batch * batch_size:]\n",
    "            target_partial = targets[batch * batch_size]\n",
    "            other_input = np.stack([inp_partial[-1]] * stripLast, axis=0)\n",
    "            other_target = np.stack([target_partial[-1]] * stripLast, axis=0)\n",
    "            inp = torch.tensor(np.stack([inp_partial, other_input], axis=0))\n",
    "            target = torch.tensor(np.stack([target_partial, other_target], axis=0))\n",
    "        inp, target = dataDict[\"scaler\"].transformBatchForEpoch((inp, target))\n",
    "        model.eval()\n",
    "        output = model(encoder_inputs = inp, decoder_inputs=target, epoch=0)\n",
    "        output = dataDict[\"scaler\"].inverse_transform(output).cpu().detach().numpy()\n",
    "        inp = dataDict[\"scaler\"].inverse_transform(inp[:,:,0,:]).cpu().detach().numpy()\n",
    "        target = dataDict[\"scaler\"].inverse_transform(target).cpu().detach().numpy()\n",
    "        datas.append(inp[:inp.shape[0] - stripLast,:,randomSensor])\n",
    "        targetVals.append(target[:target.shape[0] - stripLast,:,randomSensor])\n",
    "        preds.append(output[:output.shape[0] - stripLast, :, randomSensor])\n",
    "    return datas, targetVals, preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHours(datas, targets, preds, inputTimes, targetTimes,dataset):\n",
    "    f, ax = plt.subplots()\n",
    "    f.set_figwidth(15)\n",
    "    plt.plot(targetTimes, preds, label=\"pred\")\n",
    "    plt.plot(targetTimes, targets, label=\"target\")\n",
    "    plt.xticks(rotation=90)\n",
    "    xfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')\n",
    "    ax=plt.gca()\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"mile/h\")\n",
    "    N = preds.shape[0] / 12\n",
    "    plt.title(\"{} Hour Sample Prediction {}\".format(int(N), dataset))\n",
    "    yMin = np.min((np.min(preds)-10, np.min(targets)-10, 10))\n",
    "    yMax = np.max((np.max(preds)+10, np.max(targets)+10, 70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plotNHours(preds, targets, datas, dataset, targetTimes, N=24):\n",
    "    assert False, \"Need to fix\"\n",
    "    instance = np.random.randint(targets.shape[1])\n",
    "    sensor = np.random.randint(targets.shape[2])\n",
    "    sequenceTrueMean = []\n",
    "    sequenceTrueStd = []\n",
    "    sequenceSampleMean = []\n",
    "    sequenceSampleStd = []\n",
    "    sequenceTarget = []\n",
    "    sequenceTimes = []\n",
    "    shouldMask = []\n",
    "    maskindex = []\n",
    "    lastTime = None\n",
    "    for tStep in range(N):\n",
    "        realIndex = instance + 12 * tStep\n",
    "        if realIndex >= means.shape[1]:\n",
    "            break\n",
    "        if lastTime and inMinutes(targetTimes[realIndex, -1] - lastTime) > 5:\n",
    "            shouldMask += [True]\n",
    "        else:\n",
    "            shouldMask += [False]\n",
    "        lastTime = targetTimes[realIndex, -1]\n",
    "        maskindex += [len(sequenceTrueMean)]\n",
    "        m = means[:, realIndex, sensor]\n",
    "        std = stds[:, realIndex, sensor]\n",
    "        predSamples, sampleMean, sampleStd = getScaledSamples(m, std, dataMean, dataStd)\n",
    "        sequenceTrueMean += list(m)\n",
    "        sequenceTrueStd += list(std)\n",
    "        sequenceSampleMean += list(sampleMean)\n",
    "        sequenceSampleStd += list(sampleStd)\n",
    "        sequenceTarget += list(targets[:, realIndex, sensor])\n",
    "        sequenceTimes += list(targetTimes[realIndex])\n",
    "        \n",
    "    #f, ax = plt.subplots(2, sharex=True)\n",
    "    #f.subplots_adjust(hspace=.5)\n",
    "    \"\"\"\n",
    "    maskedSampleMean = ma.array(sequenceSampleMean)\n",
    "    maskedTarget = ma.array(sequenceTarget)\n",
    "    print(maskindex)\n",
    "    print(shouldMask)\n",
    "    print(maskedSampleMean.shape)\n",
    "    for idx, should in zip(maskindex, shouldMask):\n",
    "        if should:\n",
    "            maskedSampleMean[idx] = ma.masked\n",
    "            maskedTarget[idx] = ma.masked\n",
    "    \"\"\"\n",
    "    #print(np.max(sequenceSampleStd), sequenceTimes[np.argmax(sequenceSampleStd)])\n",
    "    #print(sequenceSampleMean)\n",
    "    f, ax = plt.subplots()\n",
    "    f.set_figwidth(15)\n",
    "    plt.plot(sequenceTimes, sequenceSampleMean, label=\"pred\")\n",
    "    plt.plot(sequenceTimes, sequenceTarget, label=\"target\")\n",
    "    plt.fill_between(sequenceTimes,np.array(sequenceSampleMean)-1.96*np.array(sequenceSampleStd), np.array(sequenceSampleMean)+1.96*np.array(sequenceSampleStd), alpha=0.5)\n",
    "    plt.xticks(rotation=90)\n",
    "    xfmt = md.DateFormatter('%Y-%m-%d %H:%M:%S')\n",
    "    ax=plt.gca()\n",
    "    ax.xaxis.set_major_formatter(xfmt)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"mile/h\")\n",
    "    plt.title(\"{} Hour Sample Prediction {}\".format(N, dataset))\n",
    "    yMin = np.min((np.min(sequenceSampleMean)-10, np.min(sequenceTarget)-10, 10))\n",
    "    yMax = np.max((np.max(sequenceSampleMean)+10, np.max(sequenceTarget)+10, 70))\n",
    "    #plt.ylim((yMin,yMax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros((5,))\n",
    "z[:z.shape[0] - 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
