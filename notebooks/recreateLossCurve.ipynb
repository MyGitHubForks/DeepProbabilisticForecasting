{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.Data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils import kld_gauss, unNormalize, load_dataset\n",
    "from model.vrnn.model import VRNN\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 10\n",
    "dataDict = load_dataset(\"./data/reformattedTraffic/\", batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEpochLoss(model, dataLoader, args, dataDict, limit=100):\n",
    "    epoch = 0\n",
    "    totalKLDLoss = 0.0\n",
    "    totalReconLoss = 0.0\n",
    "    for batch_idx, (data, target, dataT, targetT) in enumerate(dataLoader.get_iterator()):\n",
    "        if batch_idx == limit:\n",
    "            break\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"batch\", batch_idx)\n",
    "        data = torch.as_tensor(data, dtype=torch.float, device=\"cpu\").transpose(0,1)\n",
    "        target = torch.as_tensor(target, dtype=torch.float, device=\"cpu\").transpose(0,1)\n",
    "        output = model(data, target, epoch)\n",
    "        del data\n",
    "        encoder_means, encoder_stds, decoder_means, decoder_stds, prior_means, prior_stds, all_samples = output\n",
    "        # Calculate KLDivergence part\n",
    "        totalKLDLoss = 0.0\n",
    "        for enc_mean_t, enc_std_t, decoder_mean_t, decoder_std_t, prior_mean_t, prior_std_t, sample in\\\n",
    "        zip(encoder_means, encoder_stds, decoder_means, decoder_stds, prior_means, prior_stds, all_samples):\n",
    "            kldLoss = kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "            totalKLDLoss += args.kld_weight * kldLoss\n",
    "        #Calculate Prediction Loss\n",
    "        pred = torch.cat([torch.unsqueeze(y, dim=0) for y in all_samples])\n",
    "        unNPred = unNormalize(pred.detach(), dataDict[\"train_mean\"], dataDict[\"train_std\"])\n",
    "        unNTarget = unNormalize(target.detach(), dataDict[\"train_mean\"], dataDict[\"train_std\"])\n",
    "        assert pred.size() == target.size()\n",
    "        if args.criterion == \"RMSE\":\n",
    "            predLoss = torch.sqrt(torch.mean((pred - target)**2))    \n",
    "            unNormalizedLoss = torch.sqrt(torch.mean((unNPred - unNTarget)**2))\n",
    "        elif args.criterion == \"L1Loss\":\n",
    "            predLoss = torch.mean(torch.abs(pred - target))\n",
    "            unNormalizedLoss = torch.mean(torch.abs(unNPred - unNTarget))\n",
    "        totalKLDLoss += ((totalKLDLoss / args.sequence_len))\n",
    "        totalReconLoss += unNormalizedLoss\n",
    "    return (totalKLDLoss / min(dataLoader.num_batch, limit)).data.item(), (totalReconLoss / min(dataLoader.num_batch, limit)).data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelOld = torch.load(\"./save/models/model513/vrnn_full_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bunch(object):\n",
    "  def __init__(self, adict):\n",
    "    self.__dict__.update(adict)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argsStr = '{\"h_dim\": 512, \"z_dim\": 128, \"no_cuda\": true, \"no_attn\": true, \"n_epochs\": 500, \"batch_size\": 10, \"n_layers\": 2, \"initial_lr\": 0.001, \"no_lr_decay\": true, \"lr_decay_ratio\": 0.1, \"lr_decay_beginning\": 20, \"lr_decay_every\": 10, \"print_every\": 20, \"plot_every\": 1, \"criterion\": \"RMSE\", \"save_freq\": 10, \"down_sample\": 0, \"data_dir\": \"./data\", \"model\": \"vrnn\", \"weight_decay\": 5e-05, \"no_schedule_sampling\": false, \"scheduling_start\": 1.0, \"scheduling_end\": 0.0, \"tries\": 10, \"kld_weight\": 0.1, \"save_dir\": \"./save/models/model513/\", \"cuda\": false, \"_device\": \"cpu\", \"use_attn\": false, \"x_dim\": 207, \"sequence_len\": 12, \"use_schedule_sampling\": true}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argsD = json.loads(argsStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Bunch(argsD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNew = VRNN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelNew.load_state_dict(modelOld.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgkldLoss, avgreconLoss = getEpochLoss(modelNew, dataDict[\"train_loader\"], args, dataDict, limit=100)\n",
    "print(avgkldLoss, avgreconLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgkldLoss, avgreconLoss = getEpochLoss(modelNew, dataDict[\"val_loader\"], args, dataDict, limit=100)\n",
    "print(avgkldLoss, avgreconLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
